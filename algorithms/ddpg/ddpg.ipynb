{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d68cbd5",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03003813",
   "metadata": {},
   "source": [
    "Deep Deterministic Policy Gradient (DDPG) is a model-free (no transition probability) `off-policy`, and `actor-critic` algorithm that combines elements of policy gradient methods with deep Q-learning. DDPG is an extension of DQN for continuous action space. It uses `temporal difference learning` (bootstrapping) and `experience replay buffer` (off-policy) to learn the Q-value function (represented by the Critic network) via the Bellman optimality equation. \n",
    "\n",
    "The goal of reinforcement learning (RL) is to maximize the expected cumulative reward (a.k.a. expected return). In Policy Gradient algorithms, this is achieved by optimizing the policy parameters by computing the gradient ascent of the performance objective, which shifts the policy toward better actions (since the policy is deterministic, a \"probability distribution\" doesn't apply). \n",
    "\n",
    "In DDPG, the RL goal is translated to maximizing the expected Q-value estimated by the critic. To achieve that, the `Actor Network` updates its parameters computing the `gradient ascent` of the performance objective with respect to its actions. Meanwhile, the `Critic Network` minimizes its loss function (e.g., Mean Squared Error) using `gradient descent` to improve its Q-value predictions.\n",
    "\n",
    "Unlike DQN, DDPG does not use $\\epsilon$-greedy policy (exploitation) for action selection. Rather, In DDPG, the behavior policy for action selection is derived from the actions generated by the Actor network (which is a deterministic target policy) with the addition of `noise` to encourage `exploration` in the environment.\n",
    "\n",
    "- DDPG optimizes a `deterministic policy` and it is suitable for `continuous action spaces`.\n",
    "\n",
    "- DDPG uses four neural networks:\n",
    "    - The Actor network.\n",
    "    - The Critic network.\n",
    "    - The target Actor network.\n",
    "    - The target Critic network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffdb9a",
   "metadata": {},
   "source": [
    "---\n",
    "**Algorithm (Pseudocode): DDPG**\n",
    "\n",
    "1. Initialize the environment to a random state $s_t$.\n",
    "\n",
    "2. Feed the current state $s_t$ to the Actor neural network that will return an action value $a_t$ (a continuous number, not a probability, since the policy is deterministic). \n",
    "\n",
    "3. Apply a noise (typically Gaussian) to the action $a_t$ to drive the agent in the environment that will return a reward $r_t$ and the next state $s_{t+1}$.\n",
    "\n",
    "4. At each time step, store the experience/transition as a tuple ($s_t, a_t, r_{t}, s_{t+1}, d_{t}$) into the replay buffer. Where $d_{t}$ is an optional Done (boolean) value to determine whether the episode ended. This is to ensure stability.\n",
    "\n",
    "5. Update Actor network:\n",
    "\n",
    "    5.1 Sample a random state from the memory buffer and feed it to the `Actor network` $\\mu$ to get the respective action value. This action value might be different than the ones stored in the buffer.\n",
    "    \n",
    "    5.2 Feed the previous state and action pair to the `Critic network` $Q$ to get the $Q(s_i, \\alpha_i | \\theta^{Q})$ value.\n",
    "    \n",
    "    5.3 Update Actor network's parameters $\\theta^{\\mu}$ by computing the `gradient ascent` of the performance objective $J(\\pi_{\\theta})$ (a.k.a expected return under the parameterized policy) w.r.t the Actor parameters $\\theta^{\\mu}$:\n",
    "\n",
    "    \\begin{eqnarray}\n",
    "    \\theta^{\\mu}_{t+1} &=&  \\theta_t + \\alpha \\nabla_{\\theta^{\\mu}} J(\\pi_{\\theta}).\\\\\n",
    "    \\nabla_{\\theta^{\\mu}} J(\\pi_{\\theta}) &=& \\mathbb{E}_{s_t \\sim \\rho^{\\beta}}[\\nabla_{\\theta^{\\mu}} Q(s, \\alpha | \\theta^{Q})|_{s=s_t, \\alpha=\\mu(s_t|\\theta^{\\mu})}].\n",
    "    \\end{eqnarray}\n",
    "\n",
    "    Where $\\theta^{\\mu}$ and $\\theta^{Q}$ represents the Actor and Critic network's parameters, respectively. And $\\nabla_{\\theta^{\\mu}} Q(s, \\alpha | \\theta^{Q})$ is the gradient of the Critic network w.r.t the Actor parameters. \n",
    "\n",
    "6. Update Critic network:\n",
    "\n",
    "    6.1 Sample a random mini-batch of state, new states, actions and rewards from the replay buffer.\n",
    "\n",
    "    6.2 Use `target Actor network` $\\mu'$ to get actions for new states.\n",
    "\n",
    "    6.3. Feed previous actions to the `target Critic network` $Q'$ to get the target value $y_i$.\n",
    "\n",
    "    6.4. Feed state and actions to the `Critic network` to get the predicted value $Q(s_i, \\alpha_i | \\theta^{Q})$. \n",
    "\n",
    "    6.5. Update the Critic network's parameters $\\theta^{Q}$ using `gradient descent` to minimize the mean squared error loss function of the Critic network:\n",
    "\n",
    "    $$\\frac{1}{N} \\sum_i (y_i - Q(s_i, \\alpha_i | \\theta^{Q}))^2.$$\n",
    "\n",
    "    Where $y_i = r_i + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1}|\\theta^{\\mu'}| \\theta^{Q'})$ is the target value obtained in step 6.3.\n",
    "\n",
    "    6.6. Update the target networks using the soft update rule:\n",
    "\n",
    "    \\begin{align}\n",
    "    \\theta^{\\mu'} &= \\tau \\theta^{\\mu} (1-\\tau) \\theta^{\\mu'} . \\\\\n",
    "    \\theta^{Q'} &= \\tau \\theta^{Q} (1-\\tau) \\theta^{Q'}.\n",
    "    \\end{align}\n",
    "\n",
    "    Where $\\tau$ is a hyperparameter.\n",
    "\n",
    "7. Repeat until convergence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc1ae6f-4908-402f-baad-d69a6015236d",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86375437-9824-4611-88bd-b89fda60cbb0",
   "metadata": {},
   "source": [
    "[1] https://spinningup.openai.com/en/latest/algorithms/ddpg.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
