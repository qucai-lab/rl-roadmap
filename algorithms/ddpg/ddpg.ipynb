{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7327efb2-492d-4f34-ac8f-953327786b69",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <h1> \n",
    "        <a href='#'> Deep Deterministic Policy Gradient (DDPG) </a> \n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68cbd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03003813",
   "metadata": {},
   "source": [
    "Deep Deterministic Policy Gradient (DDPG) is a model-free (no transition probability) `off-policy`, and `actor-critic` algorithm that combines elements of policy gradient methods with deep Q-learning. DDPG is an extension of DQN for continuous action space. It uses `temporal difference learning` (bootstrapping) and `experience replay buffer` (off-policy) to learn the Q-value function (represented by the Critic network) via the Bellman optimality equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef34c59f-9a8d-46a4-8e42-3933d5ef1bd0",
   "metadata": {},
   "source": [
    "The goal of reinforcement learning is to maximize the expected cumulative reward (a.k.a expected return) under policy $\\pi_{\\theta}$:\n",
    "\n",
    "$$\\text{max } J(\\pi_{\\theta}) = \\text{max } \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau) | \\pi_{\\theta_t}].$$\n",
    "\n",
    "In DDPG, the RL goal is translated to learning a deterministic policy $\\mu_{\\theta}(s)$ whose actions maximize the expected Q-value estimated by the `Critic network`:\n",
    "\n",
    "$$\\text{max }_{\\theta} \\mathbb{E}_{s \\sim \\mathcal{D}}[Q_{\\phi} (s, \\mu_{\\theta}(s))].$$\n",
    "\n",
    "Where the states $s$ are sampled from a replay buffer: $s \\sim \\mathcal{D}$.\n",
    "\n",
    "To achieve the RL goal, the parameters of the `Actor Network` are updated by taking the `gradient ascent` of the Q-value function with respect to the Actor's parameters. The `Critic Network's` parameters are updated by taking the `gradient descent` of its loss function (e.g., Mean Squared Error) to improve its Q-value predictions.\n",
    "\n",
    "Unlike DQN, DDPG does not use $\\epsilon$-greedy policy (exploitation) for action selection. Rather, In DDPG, the behavior policy for action selection is derived from the actions generated by the Actor network (which is a deterministic target policy) with the addition of `noise` to encourage `exploration` in the environment.\n",
    "\n",
    "- DDPG optimizes a `deterministic policy` $\\mu_{\\theta}$ and it is suitable for `continuous action spaces`.\n",
    "\n",
    "- DDPG uses four neural networks:\n",
    "    - The Actor network.\n",
    "    - The Critic network.\n",
    "    - The target Actor network.\n",
    "    - The target Critic network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed14dd-916b-4b01-ab3a-aae1861ac9f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffdb9a",
   "metadata": {},
   "source": [
    "---\n",
    "**Algorithm (Pseudocode): Deep Deterministic Policy Gradient (adapted from Open AI)**\n",
    "\n",
    "---\n",
    "\n",
    "- Initialize the environment to a random state $s_t$. Initialize an empty replay buffer $\\mathcal{D}$. Initialize random parameters $\\theta^{\\mu}$ and $\\phi^{Q}$ for Actor and Critic networks, respectively. Set the target Actor and target Critic parameters to the main parameters: $\\theta^{\\mu'} \\leftarrow \\theta^{\\mu}$, $\\phi^{Q'} \\leftarrow \\phi^{Q}$.\n",
    "\n",
    "- **Repeat:**\n",
    "\n",
    "    - Feed the current state $s_t$ to the Actor neural network $\\mu_{\\theta}$ that will return an action value $a_t = \\mu_{\\theta} (s_t)$ (a continuous number, not a probability, since the policy is deterministic). \n",
    "\n",
    "    - Apply a noise, typically Gaussian $\\epsilon \\sim \\mathcal{N}$, to the action $a_t$ to drive the agent in the environment that will return a reward $r_t$, the next state $s_{t+1}$, and a possibly done (boolean) value $d$ that tells whether the episode has ended.\n",
    "\n",
    "    - At each time step, store the experience/transition as a tuple $B=(s_t, a_t, r_{t}, s_{t+1}, d$) into the replay buffer $\\mathcal{D}$. The replay buffer is used to ensure stability.\n",
    "\n",
    "    - **Update Critic network (Q function)**:\n",
    "\n",
    "        - Sample a random mini-batch of transitions $B=(s_t, a_t, r_t, s_{t+1},d)$ from the replay buffer $\\mathcal{D}$.\n",
    "\n",
    "        - Use `target Actor network` $\\mu_{\\theta}'$ to get the target action for the next state: $\\alpha^{\\mu'}=\\mu_{\\theta}'(s_{t+1})$.\n",
    "\n",
    "        - Feed the previous action from the target Actor to the `target Critic network` $Q'$ to get the target value: $$y_t (r, s_{t+1}, d) = r_t + \\gamma(1-d) Q_{\\phi}'(s_{t+1}, \\mu_{\\theta}'(s_{t+1})).$$\n",
    "\n",
    "        - Feed the state and action sampled from the replay buffer to the `Critic network` $Q_{\\phi}$ to get the predicted value $Q_{\\phi}(s_t, \\alpha_t)$. \n",
    "\n",
    "        - With the transitions sampled from the replay buffer ($\\tau \\sim B$), update the Critic network's parameters $\\theta^{Q}$ by computing the `gradient descent` to minimize the mean squared error loss function:\n",
    "\n",
    "        $$\\nabla_{\\phi} \\frac{1}{|B|} \\sum_{t, (\\tau \\sim B)} (y_t (r, s_{t+1}, d) - Q_{\\phi}(s_t, \\alpha_t)^2.$$\n",
    "\n",
    "    - **Update Actor network (Policy)**:\n",
    "\n",
    "        - Sample a random state $s_t$ from the memory buffer and feed it to the `Actor network` $\\mu_{\\theta}$ to get the respective action value $a_t^{\\mu}=\\mu_{\\theta}(s_t)$. This action-value might be different than the ones stored in the replay buffer.\n",
    "    \n",
    "        - Feed the previous state and action pair to the `Critic network` to get the critic value $Q_{\\phi}(s_t, \\mu_{\\theta}(s_t)$.\n",
    "    \n",
    "        - Update Actor network's parameters $\\theta^{\\mu}$ by computing the `gradient ascent` of the Q-value (with actions from the parameterized policy) w.r.t the Actor's parameters $\\theta^{\\mu}$:\n",
    "\n",
    "        \\begin{eqnarray}\n",
    "        \\theta^{\\mu}_{t+1} &=&  \\theta^{\\mu}_t + \\alpha \\nabla_{\\theta^{\\mu}} \\mathbb{E}_{s \\sim \\mathcal{D}} Q_{\\phi}(\\cdot).\\\\\n",
    "        \\nabla_{\\theta^{\\mu}} \\mathbb{E}_{s \\sim \\mathcal{D}}[Q_{\\phi} (s, \\mu_{\\theta}(s))]\n",
    "        &=& \\nabla_{\\theta^{\\mu}} \\frac{1}{|B|} \\sum_{t, s\\in B} Q_{\\phi}(s_t, \\mu_{\\theta}(s_t)).\n",
    "        \\end{eqnarray}\n",
    "   \n",
    "    - **Update the parameters of the target Actor and target Critic networks using the soft update rule**:\n",
    "\n",
    "        \\begin{align}\n",
    "        \\theta^{\\mu'} &= \\rho \\theta^{\\mu} (1-\\rho) \\theta^{\\mu'} . \\\\\n",
    "        \\phi^{Q'} &= \\rho \\phi^{Q} (1-\\rho) \\phi^{Q'}.\n",
    "        \\end{align}\n",
    "\n",
    "        Where $\\rho$ is a hyperparameter.\n",
    "\n",
    "**Repeat until convergence.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf891f2-3800-4aab-98d3-c888f48a2668",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a8ed3-fc24-44a3-b7f4-998a888e2fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cc1ae6f-4908-402f-baad-d69a6015236d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86375437-9824-4611-88bd-b89fda60cbb0",
   "metadata": {},
   "source": [
    "[1] https://spinningup.openai.com/en/latest/algorithms/ddpg.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
