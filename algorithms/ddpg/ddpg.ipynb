{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7327efb2-492d-4f34-ac8f-953327786b69",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <h1> \n",
    "        <a href='https://arxiv.org/pdf/1509.02971'> Deep Deterministic Policy Gradient (DDPG) </a> \n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68cbd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03003813",
   "metadata": {},
   "source": [
    "Deep Deterministic Policy Gradient (DDPG) is a `model-free` (no transition probability) `off-policy` (uses a replay buffer), and `actor-critic` (two neural networks) algorithm that combines elements of policy gradient methods with deep Q-learning. DDPG is an extension of DQN for `continuous action space`. \n",
    "\n",
    "- DDPG uses four neural networks:\n",
    "    - The Actor network.\n",
    "    - The Critic network.\n",
    "    - The target Actor network.\n",
    "    - The target Critic network.\n",
    "\n",
    "$ \\ $\n",
    "- DDPG has two objective loss functions: one for the Actor network (`deterministic policy gradient loss`) $L^{DPG} = J(\\pi_{\\theta}) = \\mathbb{E}_{s \\sim \\mathcal{D}}[Q_{\\phi} (s, \\mu_{\\theta}(s))$ and another for the Critic network (`mean-squared Bellman error (MSBE) loss`). While the Target Actor and Target Critic networks use the polyak averaging update rule.\n",
    "\n",
    "- DDPG maximizes $L^{DPG}$ with `gradient ascent` and minimizes MSBE with `gradient descent`.\n",
    "\n",
    "- DDPG uses `temporal difference learning` (bootstrapping) and a `experience replay buffer` (for stability) to learn an `off-policy Action-Value function` $Q(s, a)$ (estimated by the `Critic Network`) via the `Bellman optimality equation`.\n",
    "  \n",
    "- DDPG uses the aforementioned $Q(s, a)$ value function to learn a `deterministic policy` $a_t = \\mu_{\\theta} (s_t)$ estimated by the `Actor Network`.\n",
    "\n",
    "- Unlike DQN, DDPG does not use $\\epsilon$-greedy policy (exploitation) for action selection. Instead, In DDPG, the behavior policy for action selection is derived from the actions generated by the Actor network with the addition of `noise` to encourage `exploration` in the environment.\n",
    "\n",
    "- DDPG is suitable for `continuous action spaces`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf371a4e-d9c7-446d-b7d5-276a05d1f698",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# The RL Goal in DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef34c59f-9a8d-46a4-8e42-3933d5ef1bd0",
   "metadata": {},
   "source": [
    "The goal of reinforcement learning is to maximize the expected cumulative reward (a.k.a expected return) $J(\\pi_{\\theta})$ under a parameterized policy $\\pi_{\\theta}$:\n",
    "\n",
    "$$\\text{max } J(\\pi_{\\theta}) = \\text{max } \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau) | \\pi_{\\theta_t}].$$\n",
    "\n",
    "In DDPG, the RL goal is translated to learning a parameterized deterministic policy $\\mu_{\\theta}(s)$, represented by the Actor network, whose actions maximize the expected Q-value estimated by the `Critic network` $Q_{\\phi}$:\n",
    "\n",
    "$$\\text{max } J(\\pi_{\\theta})  = \\text{max }_{\\theta} \\mathbb{E}_{s \\sim \\mathcal{D}}[Q_{\\phi} (s, \\mu_{\\theta}(s))].$$\n",
    "\n",
    "Where the state $s$ is sampled from a replay buffer: $s \\sim \\mathcal{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe459d-0dad-4fc6-8297-c5bda6ce0f12",
   "metadata": {},
   "source": [
    "In DDPG, the parameters of the `Critic Network` are updated by taking the `gradient descent` of the mean-squared Bellman error (MSBE) loss function to improve its Q-value predictions. While the parameters of the `Actor Network` $\\mu_{\\theta}$ are updated by taking the `gradient ascent` of the Q-value function with respect to the Actor's parameters:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\theta^{\\mu}_{t+1} &=& \\theta^{\\mu}_t + \\alpha \\nabla_{\\theta} J(\\mu_{\\theta})\\\\\n",
    "&=& \\theta^{\\mu}_t + \\alpha \\nabla_{\\theta^{\\mu}} \\mathbb{E}_{s \\sim \\mathcal{D}}[Q_{\\phi} (s, \\mu_{\\theta}(s))] \\\\\n",
    "&=& \\theta^{\\mu}_t + \\alpha \\mathbb{E}_{s \\sim \\mathcal{D}} \\Bigg[ \\nabla_{\\theta^{\\mu}} Q_{\\phi} (s, \\mu_{\\theta}(s)) \\Bigg].\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c1cc0-2708-4b28-ab02-9e634d74991f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Deriving the Deterministic Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc0338-3671-4d3f-bd75-f83d4250e120",
   "metadata": {},
   "source": [
    "The general form of the Policy Gradient, which follows from the **Policy Gradient Theorem** (Sutton et al., 1999), is: \n",
    "\n",
    "$$ \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\Bigg[ \\sum_{t=0}^T \\nabla_{\\theta} log \\left( \\pi_{\\theta} (a_t | s_t) \\right) \\Phi_t \\Bigg].$$\n",
    "\n",
    "In the case of DDPG, $\\Phi = Q_{\\phi} (s, \\mu_{\\theta}(s))$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb98052-bc39-48d5-b18d-4d4dbe9e4694",
   "metadata": {},
   "source": [
    "The **Policy Gradient Theorem** in DPPG can be reformulated as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "        \\nabla_{\\theta^{\\mu}} \\Bigg( J(\\mu_{\\theta}) \\Bigg) &=& \\nabla_{\\theta^{\\mu}} \\Bigg( \\int  \\underbrace{\\mathbb{P}(\\tau | \\theta)}_{\\text{Trajectory prob.}} Q_{\\phi} (s, \\mu_{\\theta}(s)) \\Bigg) \\\\\n",
    "        &=& \\nabla_{\\theta^{\\mu}} \\Bigg( \\mathbb{E}_{s \\sim \\mathcal{D}}[Q_{\\phi} (s, \\mu_{\\theta}(s))] \\Bigg)\\\\\n",
    "        &=&  \\mathbb{E}_{s \\sim \\mathcal{D}} \\Bigg[ \\nabla_{\\theta^{\\mu}} Q_{\\phi} (s, \\mu_{\\theta}(s)) \\Bigg] \\\\\n",
    "        &\\underbrace{=}_{\\text{after chain rule}}&  \\mathbb{E}_{s \\sim \\mathcal{D}} \\Bigg[ \\nabla_a Q_{\\phi} (s, a)|_{a=\\mu_{\\theta}(s)} \\nabla_{\\theta^{\\mu}} \\mu_{\\theta}(s) \\Bigg] \\\\\n",
    "        &\\approx&  \\underbrace{\\frac{1}{|B|} \\sum_{t, s \\in \\mathcal{D}}}_{\\text{empirical average}} \\Bigg[ \\nabla_a Q_{\\phi} (s_t, a_t)|_{a_t=\\mu_{\\theta}(s_t)} \\nabla_{\\theta^{\\mu}} \\mu_{\\theta}(s_t) \\Bigg].\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Where $|B|$ is the number of transitions stored in the replay buffer $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed14dd-916b-4b01-ab3a-aae1861ac9f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffdb9a",
   "metadata": {},
   "source": [
    "---\n",
    "**Algorithm (Pseudocode): Deep Deterministic Policy Gradient (adapted from Open AI)**\n",
    "\n",
    "---\n",
    "\n",
    "- Initialize the environment to a random state $s_t$. Initialize an empty replay buffer $\\mathcal{D}$. Initialize random parameters $\\theta^{\\mu}$ and $\\phi^{Q}$ for Actor and Critic networks, respectively. Set the target Actor and target Critic parameters to the main parameters: $\\theta^{\\mu_{targ}} \\leftarrow \\theta^{\\mu}$, $\\phi^{Q_{targ}} \\leftarrow \\phi^{Q}$.\n",
    "\n",
    "- **Repeat:**\n",
    "\n",
    "    - Feed the current state $s_t$ to the Actor neural network $\\mu_{\\theta}$ that will return an action value $a_t = \\mu_{\\theta} (s_t)$ (a continuous number, not a probability, since the policy is deterministic). \n",
    "\n",
    "    - Apply a noise, typically Gaussian $\\epsilon \\sim \\mathcal{N}$, to the action $a_t$.\n",
    "    \n",
    "    - Drive the agent with action $a_t$ in the environment that will return a reward $r_t$, the next state $s_{t+1}$, and a possibly done (boolean) value $d$ that tells whether the episode has ended.\n",
    " \n",
    "    - Store the experience/transition as a tuple $B=(s_t, a_t, r_{t}, s_{t+1}, d$) into the replay buffer $\\mathcal{D}$. The replay buffer is used to ensure stability.\n",
    "\n",
    "    - If the next state $s_{t+1}$ is the terminal state, reset the environment.\n",
    "\n",
    "    - **Update Critic network (Q function)**:\n",
    "\n",
    "        - Sample a random mini-batch of transitions $B=(s_t, a_t, r_t, s_{t+1},d)$ from the replay buffer $\\mathcal{D}$.\n",
    "\n",
    "        - Use `target Actor network` $\\mu_{\\theta_{targ}}$ to get the target action for the next state: $\\alpha_{\\mu_{targ}}=\\mu_{\\theta_{targ}}(s_{t+1})$.\n",
    "\n",
    "        - Feed the previous action from the target Actor to the `target Critic network` $Q_{\\phi_{targ}}$ to get the target value: $$y_t (r, s_{t+1}, d) = r_t + \\gamma(1-d) Q_{\\phi_{targ}}(s_{t+1}, \\mu_{\\theta_{targ}}(s_{t+1})).$$\n",
    "\n",
    "        - Feed the state and action sampled from the replay buffer to the `Critic network` $Q_{\\phi}$ to get the predicted value $Q_{\\phi}(s_t, \\alpha_t)$. \n",
    "\n",
    "        - With the transitions sampled from the replay buffer ($\\tau \\sim B$), update the Critic network's parameters $\\theta^{Q}$ by computing the `gradient descent` to minimize the `mean-squared Bellman error (MSBE) loss function`:\n",
    "\n",
    "        $$\\nabla_{\\phi} \\frac{1}{|B|} \\sum_{t, (\\tau \\sim B)} \\left(y_t (r, s_{t+1}, d) - Q_{\\phi}(s_t, \\alpha_t) \\right)^2.$$\n",
    "\n",
    "    - **Update Actor network (Policy)**:\n",
    "\n",
    "        - Sample a random state $s_t$ from the memory buffer and feed it to the `Actor network` $\\mu_{\\theta}$ to get the respective action value $a_t^{\\mu}=\\mu_{\\theta}(s_t)$. This action-value might be different than the ones stored in the replay buffer.\n",
    "    \n",
    "        - Feed the previous state and action pair to the `Critic network` to get the critic value $Q_{\\phi}(s_t, \\mu_{\\theta}(s_t)$.\n",
    "    \n",
    "        - Update Actor network's parameters $\\theta^{\\mu}$ by computing the `gradient ascent` of the $L^{DPG}$ loss (performance objective) w.r.t the Actor's parameters $\\theta^{\\mu}$:\n",
    "\n",
    "        \\begin{eqnarray}\n",
    "        \\theta^{\\mu}_{t+1} &=&  \\theta^{\\mu}_t + \\alpha \\nabla_{\\theta^{\\mu}} J(\\mu_{\\theta}).\\\\\n",
    "        \\nabla_{\\theta^{\\mu}} J(\\mu_{\\theta})\n",
    "        &=& \\nabla_{\\theta^{\\mu}} \\Bigg( \\mathbb{E}_{s \\sim \\mathcal{D}}[Q_{\\phi} (s, \\mu_{\\theta}(s))] \\Bigg) \\\\\n",
    "        &=& \\nabla_{\\theta^{\\mu}} \\frac{1}{|B|} \\sum_{t, s\\in B} Q_{\\phi}(s_t, \\mu_{\\theta}(s_t))\\\\\n",
    "        &=& \\frac{1}{|B|} \\sum_{t, s \\in \\mathcal{D}} \\Bigg[ \\nabla_a Q_{\\phi} (s_t, a_t)|_{a_t=\\mu_{\\theta}(s_t)} \\nabla_{\\theta^{\\mu}} \\mu_{\\theta}(s_t) \\Bigg]\n",
    "        \\end{eqnarray}\n",
    "   \n",
    "    - **Update the parameters of the target Actor and target Critic networks via polyak averaging**:\n",
    "\n",
    "        \\begin{align}\n",
    "        \\theta^{\\mu_{targ}} &= \\rho \\theta^{\\mu} + (1-\\rho) \\theta^{\\mu_{targ}} . \\\\\n",
    "        \\phi^{Q_{targ}} &= \\rho \\phi^{Q} + (1-\\rho) \\phi^{Q_{targ}}.\n",
    "        \\end{align}\n",
    "\n",
    "        Where $\\rho$ is a hyperparameter.\n",
    "\n",
    "**until convergence or until a maximum number of episodes.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf891f2-3800-4aab-98d3-c888f48a2668",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a8ed3-fc24-44a3-b7f4-998a888e2fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cc1ae6f-4908-402f-baad-d69a6015236d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86375437-9824-4611-88bd-b89fda60cbb0",
   "metadata": {},
   "source": [
    "[1] [Continuous Control With Deep Reinforcement Learning, Lillicrap et al. 2016.](https://arxiv.org/pdf/1509.02971)\n",
    "\n",
    "[2] [Deterministic Policy Gradient Algorithms, Silver et al. 2014.](http://proceedings.mlr.press/v32/silver14.pdf)\n",
    "\n",
    "[3] https://spinningup.openai.com/en/latest/algorithms/ddpg.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
