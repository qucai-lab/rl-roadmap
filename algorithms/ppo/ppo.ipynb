{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cae3de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div align='center'>\n",
    "    <h1> \n",
    "        <a href='https://arxiv.org/abs/1707.06347'> Proximal Policy Optimization (PPO) </a> \n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1f289",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9626d-abb6-4383-b7b8-d65ef79690d5",
   "metadata": {},
   "source": [
    "Proximal Policy Optimization (PPO) is an `Actor-Critic` (AC), `online` (learns from trajectories collected during run time), and `on-policy` algorithm (uses only trajectories collected with the latest policy). Contrary to VPG and A2C/A3C algorithms, PPO indirectly optimizes the Policy performance objective $J(\\pi_{\\theta})$ and, instead, maximizes a surrogate objective function. Unlike second-order methods (e.g., TRPO), it does not compute the Hessian matrix (second-order derivatives). PPO is a family of first-order methods, meaning that only first-order derivatives (gradients) are considered for optimization. PPO is designed to keep the updated policy close to the old policy.\n",
    "\n",
    "- PPO optimizes a `stochastic policy` and is suitable for both `continuous and discrete action spaces`.\n",
    "  \n",
    "PPO has to main variants:\n",
    "\n",
    "1) PPO-Penalty: uses KL-divergence with a penalty term in the objective function. The penalty coefficient is automatically adjusted during training.\n",
    "\n",
    "2) PPO-Clip: improves upon the TRPO algorithm. In TRPO, the KL-divergence term (constraint) in the objective function, which prevents the old policy $\\pi_{\\theta_{\\text{old}}}$ to be far from the updated (new) policy $\\pi(\\theta)$, introduces an overhead that is circumvented in PPO by a clipped surrogate objective. The KL-divergence term is eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3baed-3112-4b97-8ca2-2bee8ca8c595",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Is PPO a Policy Gradient Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46fe719-ff1e-4f61-ad6f-cd3dd93244f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "[\"PPO is often referred to as a policy gradient algorithm, though this is slightly inaccurate.\" —Open AI.](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)\n",
    "\n",
    "[\"Open AI implementation of PPO makes use of Generalized Advantage Estimation for computing the policy gradient.\" —Open AI.](https://spinningup.openai.com/en/latest/algorithms/ppo.html)\n",
    "\n",
    "PPO does use policy gradients in its optimization process, however, it introduces additional mechanisms that set it apart from \"pure\" policy gradient methods:\n",
    "\n",
    "1) Clipping: PPO uses a surrogate objective loss function with a clipped probability ratio, which serves as a soft trust region constraint. This helps to limit the size of policy updates, improving stability. This addresses a key issue with standard policy gradient methods, where large policy updates can destabilize the learning process.\n",
    "\n",
    "2) Batch update: PPO performs multiple optimization steps on the same batch of data for sample efficiency. This is different from traditional policy gradient methods, which usually update the policy using data only once per batch.\n",
    "\n",
    "It is more precise to describe PPO as an advanced policy optimization algorithm that builds upon and extends policy gradient techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc378c-2d61-4f85-8904-0b9caf11b2fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Actor loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc72ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "For the Actor network, the clipped surrogate objective loss function is defined as the minimum between two objectives, the no clipping or penalty objective which is the default in policy gradient methods, and its clipped version:\n",
    "\n",
    "$$L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_k, \\theta) \\equiv \\hat{\\mathbb{E}}_t \\Bigg[\\text{min}\\Bigg(r_t(\\theta)A^{\\pi_{\\theta_k}}(s,a), \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A^{\\pi_{\\theta_k}}(s,a) \\Bigg) \\Bigg],$$\n",
    "\n",
    "where $\\epsilon$ is a parameter used to clip (constraint) the value of the probability ratio $$r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_k}(a_t | s_t)},$$ in the range $1-\\epsilon$ to $1+\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4c73d-67aa-4c7d-807c-8d628c4c5544",
   "metadata": {},
   "source": [
    "The advantage function, denoted by $$A^{\\pi_{\\theta_k}}(s_t,a_t) = Q^{\\pi_{\\theta_k}}(s_t, a_t) - V^{\\pi_{\\theta_k}}(s_t),$$\n",
    "\n",
    "is the difference between the expected return (Action-Value function) and the baseline estimate. The baseline is the State Value function that gives a noisy estimate of the return. If the value of the advantage function is positive then the gradient is positive and the likelihood of the selected actions (action probabilities) increases, otherwise the gradient is negative and the actions are discouraged.\n",
    "\n",
    "This advantage is calculated as follows:\n",
    "\n",
    "$$A^{\\pi_{\\theta_k}}(s_t,a_t) = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1},$$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\delta_t = r_t + \\gamma V(s_{t+1})-V(s_t)$.\n",
    "- $\\gamma$ is the discount factor (~0.99).\n",
    "- $\\lambda$ is a parameter used to reduce variance (~0.95)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37190ee-5603-44a5-b4ea-fecbe50bfe99",
   "metadata": {},
   "source": [
    "A simplified version of the PPO-Clip objective, i.e., the Actor loss is [1]:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_k, \\theta) =  \\hat{\\mathbb{E}}_t \\Bigg[\\text{min}\\Bigg(r_t(\\theta)A^{\\pi_{\\theta_k}}(s,a), g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a)) \\Bigg)\\Bigg],\n",
    "\\end{eqnarray}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{eqnarray}\n",
    "g(\\epsilon, A) = \n",
    "\\begin{cases}\n",
    "(1+\\epsilon)A, A \\geq 0. \\\\\n",
    "(1-\\epsilon)A, A < 0.\n",
    "\\end{cases}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a71a8-48e8-4ff4-9772-f3960d1bab1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Critic loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d3e22-edc6-4a89-bdb9-04c83f333abf",
   "metadata": {},
   "source": [
    "The Critic network loss function is:\n",
    "\n",
    "$$L_{critic}^{\\text{VF}}(\\theta) = (V_{\\theta}(s_t) - V_t^{targ})^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2f7d1-92f6-40ce-b43c-89bf653c32dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Final loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b5fedc-7580-416e-8cbe-71d7e368bf70",
   "metadata": {},
   "source": [
    "The final objective is the sum of the Critic loss and the Actor loss:\n",
    "\n",
    "$$L_t^{\\text{CLIP+VF+S}}(s, a, \\theta_k, \\theta) =: L_t^{\\text{PPO}}(s, a, \\theta_k, \\theta) = \\hat{\\mathbb{E}}_t \\Bigg[L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_k, \\theta) -c_1 L_{critic}^{VF}(\\theta)+c_2 S[\\pi_{\\theta}](s_t) \\Bigg],$$\n",
    "\n",
    "where $S$ is an entropy term used to ensure enough exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b0d80-e4db-4736-a487-ba12c0ee2af4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccaece-d009-4e72-af50-252b2c6cee69",
   "metadata": {},
   "source": [
    "The following cell presents a pseudocode for the PPO-Clip variant version of the PPO algorithm with the clipped objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a42062",
   "metadata": {},
   "source": [
    "---\n",
    "**Algorithm (Pseudocode): Proximal Policy Optimization - Clip version (adapted from [Open AI](https://spinningup.openai.com/en/latest/algorithms/ppo.html#:~:text=PPO%20is%20an%20on%2Dpolicy,discrete%20or%20continuous%20action%20spaces.))**\n",
    "\n",
    "---\n",
    "\n",
    "- Input: initialize policy parameters $\\theta_0$, and initial value function parameters $\\phi_0$.\n",
    "\n",
    "- for iteration $= 0, 1, 2, \\dots$ do:\n",
    "\n",
    "    - for actor=$1,2,\\cdots, N$ do:\n",
    "\n",
    "        - Collect a set of trajectories $\\mathcal{H}_t \\doteq \\mathcal{D}_k\\doteq\\{\\tau_i\\} = (s_0, a_0, r_0, \\cdots , s_T, a_{T}, r_T)$ by executing the current policy $\\pi_k = \\pi(\\theta_k)=\\pi_{\\theta_{\\text{old}}}$ in the environment for $T$ time steps.\n",
    "        - For each trajectory, compute: \n",
    "            - the reward-to-go $\\hat{\\mathcal{R}}_t = \\sum_{t'=t}^T R(s_t', a_t', s_{t'+1})$, and\n",
    "            - the advantage estimates $\\hat{A}_1, \\dots, \\hat{A}_T$ (using any advantage estimation method) based on the current  on-policy state value function $V_{\\phi_k}$ used as the baseline: $$\\hat{A}_t = Q^{\\pi_{\\theta}}(s_t, a_t) - V^{\\pi_{\\theta}}(s_t) \\in {\\rm I\\!R}.$$ \n",
    "    - end for.\n",
    "    - Update the Policy by maximizing the PPO-Clip objective, typically via stochastic `gradient ascent` with Adam: $$\\theta_{k+1} =  \\text{arg max}_{\\theta } \\hat{\\mathbb{E}}_{s,a \\sim \\pi_{\\theta_k}} [L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_k, \\theta)] = \\text{arg max}_{\\theta}\\frac{1}{|\\mathcal{D}_k|T}\\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^T \\text{min } \\Bigg(\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_k}(a_t | s_t)}A^{\\pi_{\\theta_k}} (s_t, a_t), g(\\epsilon, A^{\\pi_{\\theta_k}} (s_t, a_t)) \\Bigg).$$\n",
    "    - Fit the value function by regression on mean-squared error, via some `gradient descent` algorithm, minimizing $(V_\\phi(s_t) - \\hat{\\mathcal{R}}_t)^2$ summed over all trajectories and time steps: \n",
    "    \n",
    "$$\\phi_{k+1} = \\underset{\\phi}{\\operatorname{arg\\,min}} \\ \\frac{1}{|\\mathcal{D}_k|T}\\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^T \\left(V_\\phi(s_t) - \\hat{\\mathcal{R}}_t \\right)^2.$$ \n",
    "\n",
    "    \n",
    "- end for.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611a320-4126-482b-a6bf-8834b195a568",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a2e61",
   "metadata": {},
   "source": [
    "Following the paper's implementation, the Policy network is an MLP with two hidden layers, 64 neurons, and tanh activation function. The $c_1$ coefficient in the final objective function (critic loss + actor loss) is discarted since the policy and the value networks do not share parameters between each other. And the entropy term used to ensure enough exploration is also ignored.\n",
    "\n",
    "Here, we define:\n",
    "\n",
    "$$L_t^{\\text{VF}}(\\theta) =: L_{critic} \\equiv MSE(\\hat{A}_t(s_t,a_t) + CriticValue_{mem} - CriticValue_{net}).$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e95330de-395a-4ba5-bf89-33f9546b4757",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87ccd3-daab-46bb-9faa-b8624e6d1cdd",
   "metadata": {},
   "source": [
    "[1] https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "[2] https://drive.google.com/file/d/1PDzn9RPvaXjJFZkGeapMHbHGiWWW20Ey/view"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
