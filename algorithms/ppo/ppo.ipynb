{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cae3de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div align='center'>\n",
    "    <h1> \n",
    "        <a href='https://arxiv.org/abs/1707.06347'> Proximal Policy Optimization (PPO) </a> \n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1f289",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9626d-abb6-4383-b7b8-d65ef79690d5",
   "metadata": {},
   "source": [
    "Proximal Policy Optimization (PPO) is an `Actor-Critic` (AC), `online` (learns from trajectories collected during run time), and `on-policy` algorithm (uses only trajectories collected with the latest policy). \n",
    "\n",
    "- PPO has two neural networks:\n",
    "    - The Actor Network $\\pi_{\\theta} (s)$.\n",
    "    - The Critic Network $V_{\\phi} (s)$.\n",
    "\n",
    "$\\ $\n",
    "- PPO has two objective loss functions: one for the Actor network (`policy surrogate objective loss function`) $L_\\text{actor}^{\\text{CLIP}}$ and another for the Critic network (`value function error term`) $L_{critic}^{\\text{VF}}$.\n",
    "\n",
    "- Contrary to VPG and A2C/A3C algorithms, PPO indirectly maximizes the Policy performance objective $J(\\pi_{\\theta})$ and, instead, maximizes $L_\\text{actor}^{\\text{CLIP}}$ using `stochastic gradient ascent` to optimize a `stochastic policy` $a_t \\sim \\pi_{\\theta}(a_t | s_t)$.\n",
    "\n",
    "- Unlike second-order methods (e.g., TRPO), it does not compute the Hessian matrix (second-order derivatives). PPO is a family of first-order methods, meaning that only first-order derivatives (gradients) are considered for optimization. PPO is designed to keep the updated policy close to the old policy.\n",
    "\n",
    "- PPO is suitable for both `continuous and discrete action spaces`.\n",
    "\n",
    "PPO has two main variants:\n",
    "\n",
    "1) PPO-Penalty: uses KL-divergence with a penalty term in the objective function. The penalty coefficient is automatically adjusted during training.\n",
    "\n",
    "2) PPO-Clip: improves upon the TRPO algorithm. In TRPO, the KL-divergence term (constraint) in the objective function, which prevents the old policy $\\pi_{\\theta_{\\text{old}}}$ to be far from the updated (new) policy $\\pi_{\\theta}$, introduces an overhead that is circumvented in PPO by a clipped surrogate objective. The KL-divergence term is eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3baed-3112-4b97-8ca2-2bee8ca8c595",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Is PPO a Policy Gradient Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1aee44-9cd4-451d-bf7f-2eb46757f973",
   "metadata": {},
   "source": [
    "[\"PPO is often referred to as a policy gradient algorithm, though this is slightly inaccurate.\" —Open AI.](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)\n",
    "\n",
    "[\"Open AI implementation of PPO makes use of Generalized Advantage Estimation for computing the policy gradient.\" —Open AI.](https://spinningup.openai.com/en/latest/algorithms/ppo.html)\n",
    "\n",
    "PPO does use policy gradients in its optimization process, however, it introduces additional mechanisms that set it apart from \"pure\" policy gradient methods:\n",
    "\n",
    "1) Clipping: PPO uses a surrogate objective loss function with a clipped probability ratio, which serves as a soft trust region constraint. This helps to limit the size of policy updates, improving stability. This addresses a key issue with standard policy gradient methods, where large policy updates can destabilize the learning process.\n",
    "\n",
    "2) Batch update: PPO performs multiple gradient updates on the same minibatch of data for sample efficiency. This is different from traditional policy gradient methods that perform one gradient update per data sample. —\"Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates.\" [1].\n",
    "\n",
    "It is more precise to describe PPO as an advanced policy optimization algorithm that builds upon and extends policy gradient techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b599e8-4a0c-4251-9040-e4f67c216cdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# The RL Goal in PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42efa16-65ab-48c5-b0db-7642002c13af",
   "metadata": {},
   "source": [
    "The goal of reinforcement learning is to maximize the expected cumulative reward (a.k.a expected return) $J(\\pi)$ under a policy $\\pi$:\n",
    "\n",
    "$$\\text{max } J(\\pi) = \\text{max } \\mathbb{E}_{\\tau \\sim \\pi}[\\mathcal{R}(\\tau) | \\pi].$$\n",
    "\n",
    "In PPO, the RL goal is translated to learning a parameterized stochastic policy $\\pi_{\\theta}(s)$, represented by the Actor network, whose actions maximize the clipped surrogate objective loss function $L_\\text{actor}^{\\text{CLIP}}$:\n",
    "\n",
    "$$\n",
    "\\underset{\\theta}{\\operatorname{max}} \\ L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_{old}, \\theta)\n",
    "= \\underset{\\theta}{\\operatorname{max}} \\\n",
    "\\hat{\\mathbb{E}}_{s, a \\sim \\pi_{\\theta_{old}}} \\Bigg[\\text{min}\\Bigg(r(\\theta) A^{\\pi_{\\theta_{old}}}(s,a), \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon)A^{\\pi_{\\theta_{old}}}(s,a) \\Bigg) \\Bigg]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b5265-52e9-48e2-8649-8be05b715567",
   "metadata": {},
   "source": [
    "In PPO, the parameters of the `Actor Network` are updated by computing the `stochastic gradient ascent` of the clipped surrogate objective loss:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\theta_{k+1} &=& \\theta_k + \\alpha \\nabla_{\\theta} L_\\text{actor}^{\\text{CLIP}} \\\\\n",
    "&=& \\theta_k + \\alpha  \\hat{\\mathbb{E}}_{s, a \\sim \\pi_{\\theta_{old}}} \\Bigg[\\nabla_{\\theta} \\ \\text{min}\\Bigg(r(\\theta) A^{\\pi_{\\theta_{old}}}(s,a), \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon)A^{\\pi_{\\theta_{old}}}(s,a) \\Bigg) \\Bigg]\n",
    "\\end{eqnarray}\n",
    "\n",
    "The derivative of the min function splits into two cases based on whether $r(\\theta)$ or the clipped term is smaller.\n",
    "\n",
    "In the limit of infinitesimal steps ($\\alpha \\rightarrow 0$), this corresponds to solving:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\theta_{k+1} = \\underset{\\theta}{\\operatorname{arg\\,max}} \\ L_\\text{actor}^{\\text{CLIP}}.\n",
    "\\end{eqnarray}\n",
    "\n",
    "For computing samples, it is useful to use the empirical average (sample mean):\n",
    "\n",
    "$$\n",
    "L_\\text{actor}^{\\text{CLIP}} = \\frac{1}{|\\mathcal{D}_k|T}\\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^T \\text{min } \\Bigg(\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_k}(a_t | s_t)}A^{\\pi_{\\theta_k}} (s_t, a_t), g(\\epsilon, A^{\\pi_{\\theta_k}} (s_t, a_t)) \\Bigg).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a58aad-db38-4fb7-9569-2bba2753f28f",
   "metadata": {},
   "source": [
    "While the parameters of the `Critic Network` are updated by computing the `gradient descent` of the mean-squared error loss:\n",
    "\n",
    "$$(V_{\\phi_k}(s_t) - \\hat{\\mathcal{R}}_t)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd938c7-717e-4469-ba82-81a9f7eeda0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Objective Loss Functions in PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc378c-2d61-4f85-8904-0b9caf11b2fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Actor loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc72ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The Actor network loss function is the clipped surrogate objective loss function defined as the minimum between two objectives, the no clipping or penalty objective, which is the default in policy gradient methods, and its clipped version:\n",
    "\n",
    "$$L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_{old}, \\theta) \\equiv \\hat{\\mathbb{E}}_{s, a \\sim \\pi_{\\theta_{old}}} \\Bigg[\\text{min}\\Bigg(r(\\theta) A^{\\pi_{\\theta_{old}}}(s,a), \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon)A^{\\pi_{\\theta_{old}}}(s,a) \\Bigg) \\Bigg].$$\n",
    "\n",
    "Where $$r(\\theta) \\equiv \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)},$$ \n",
    "\n",
    "is the probability ratio of the new policy $\\pi_{\\theta}$ to the old policy $\\pi_{\\theta_{old}}$ at timestep $t$. If $r(\\theta) > 1$, the new policy dominates.\n",
    "\n",
    "The clip function constrains the value of the probability ratio $r(\\theta)$ to the range $1-\\epsilon$ to $1+\\epsilon$, where $\\epsilon$ is a hyperparameter. Considering $\\epsilon = 0.2$, then $r(\\theta) \\in [0.8, 1.2]$. This constraint avoids large policy updates making sure the new policy is not too far away from the old one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4c73d-67aa-4c7d-807c-8d628c4c5544",
   "metadata": {},
   "source": [
    "The advantage function, denoted by $$A(s_t,a_t) = Q(s_t, a_t) - V(s_t),$$\n",
    "\n",
    "is the difference between the expected return (Action-Value function) and the baseline estimate. The baseline is the On-Policy State Value function that gives a noisy estimate of the return. If the value of the advantage function is positive then the gradient is positive and the likelihood of the selected actions (action probabilities) increases, otherwise the gradient is negative and the actions are discouraged.\n",
    "\n",
    "This advantage is calculated as follows:\n",
    "\n",
    "$$A(s_t,a_t) = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1}.$$\n",
    "\n",
    "Legend: \n",
    "\n",
    "- $\\delta_t = r_t + \\gamma V(s_{t+1})-V(s_t)$.\n",
    "- $\\gamma$ is the discount factor (~0.99).\n",
    "- $\\lambda$ is a parameter used to reduce variance (~0.95)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37190ee-5603-44a5-b4ea-fecbe50bfe99",
   "metadata": {},
   "source": [
    "A simplified version of the PPO-Clip objective (Actor loss) is [3]:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_{old}, \\theta) =  \\hat{\\mathbb{E}}_{s, a \\sim \\pi_{\\theta_{old}}} \\Bigg[\\text{min}\\Bigg(r(\\theta)A^{\\pi_{\\theta_{old}}}(s,a), g(\\epsilon, A^{\\pi_{\\theta_{old}}}(s,a)) \\Bigg)\\Bigg],\n",
    "\\end{eqnarray}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{eqnarray}\n",
    "g(\\epsilon, A) = \n",
    "\\begin{cases}\n",
    "(1+\\epsilon)A, A \\geq 0. \\\\\n",
    "(1-\\epsilon)A, A < 0.\n",
    "\\end{cases}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a71a8-48e8-4ff4-9772-f3960d1bab1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Critic loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d3e22-edc6-4a89-bdb9-04c83f333abf",
   "metadata": {},
   "source": [
    "The Critic network objective loss function (a.k.a value function error term) is:\n",
    "\n",
    "$$L_{critic}^{\\text{VF}}(\\phi) = (V_{\\phi}(s_t) - V_t^{targ})^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2f7d1-92f6-40ce-b43c-89bf653c32dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Final loss (shared parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b5fedc-7580-416e-8cbe-71d7e368bf70",
   "metadata": {},
   "source": [
    "When implementing a neural network that shares parameters between the Policy (Actor network) and the Value function (Critic network), the final objective loss function is the sum of the Critic loss and the Actor loss:\n",
    "\n",
    "$$L_t^{\\text{CLIP+VF+S}}(s, a, \\theta_{old}, \\theta) =: L^{\\text{PPO}}(s, a, \\theta_{old}, \\theta) = \\hat{\\mathbb{E}} \\Bigg[L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_{old}, \\theta) -c_1 L_{critic}^{VF}(\\theta)+c_2 S[\\pi_{\\theta}](s) \\Bigg],$$\n",
    "\n",
    "where $c_1$ and $c_2$ are scalar coefficients, and $S$ is an entropy term (bonus) used to ensure enough exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b0d80-e4db-4736-a487-ba12c0ee2af4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccaece-d009-4e72-af50-252b2c6cee69",
   "metadata": {},
   "source": [
    "The following cell presents a pseudocode for the PPO-Clip variant version of the PPO algorithm with Generalized Advantage Estimate (GAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a42062",
   "metadata": {},
   "source": [
    "---\n",
    "**Algorithm (Pseudocode): Proximal Policy Optimization - Clip version with GAE (adapted from [Open AI](https://spinningup.openai.com/en/latest/algorithms/ppo.html#:~:text=PPO%20is%20an%20on%2Dpolicy,discrete%20or%20continuous%20action%20spaces.))**\n",
    "\n",
    "---\n",
    "\n",
    "- Input: initialize policy parameters $\\theta_0$, and initial value function parameters $\\phi_0$.\n",
    "\n",
    "- **for** episode $k = 0, 1, 2, \\dots$ do:\n",
    "\n",
    "    - **for** parallel actor $= 1, 2, \\cdots, N$ do:\n",
    "\n",
    "        - Collect a set of trajectories from multiple parallel actors $\\mathcal{D}_k\\doteq\\{\\tau_i\\} = (s_0, a_0, r_0, \\cdots , s_T, a_{T}, r_T)$ by executing the current policy $\\pi_k = \\pi(\\theta_k)$ in the environment for $T$ time steps.\n",
    "          \n",
    "        - **for** each trajectory, compute: \n",
    "            - the reward-to-go $\\hat{\\mathcal{R}}_t = \\sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1})$, and\n",
    "            - the Generalized Advantage Estimate $A^{\\text{GAE}}_t$ (using any advantage estimation method such as `Temporal Difference (TD)`) based on the current on-policy state value function $V_{\\phi_k}$ as the baseline to reduce sample variance in the gradient estimate: \\begin{eqnarray}A_t^{\\text{GAE}}(s_t, a_t) = \\sum_{i=0}^{T} (\\gamma \\lambda)^i \\delta_{t+i} = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + \\cdots + = \\delta_t + (\\gamma \\lambda) A^{\\text{GAE}}_{t+1} \\\\ \\delta_t = r_t + \\gamma V_{\\phi_k}(s_{t+1}) - V_{\\phi_k}(s_t)\\end{eqnarray}\n",
    "        - **end for**.\n",
    "    - **end for**.\n",
    "\n",
    "    - **if $K$ episodes:**\n",
    "        - Update the Policy by maximizing the PPO-Clip surrogate objective w.r.t $\\theta$, typically via stochastic `gradient ascent` with Adam, for $K$ episodes and minibatch size $M \\leq N T$: $$\\theta_{k+1} = \\underset{\\theta}{\\operatorname{arg\\,max}} \\ \\frac{1}{|\\mathcal{D}_k|T}\\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^T \\text{min } \\Bigg(\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_k}(a_t | s_t)}A^{\\pi_{\\theta_k}} (s_t, a_t), g(\\epsilon, A^{\\pi_{\\theta_k}} (s_t, a_t)) \\Bigg).$$\n",
    "          \n",
    "        - Re-fit (learn) the on-policy state value function $V_{\\phi_k}$ by regression on mean-squared error, via some `gradient descent` algorithm, minimizing $(V_{\\phi_k}(s_t) - A^{\\text{GAE}}_t)^2$ summed over all trajectories and time steps: \n",
    "        \n",
    "    $$\\phi_{k+1} = \\underset{\\phi}{\\operatorname{arg\\,min}} \\ \\frac{1}{|\\mathcal{D}_k|T}\\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^T \\left(V_{\\phi_k}(s_t) - A^{\\text{GAE}}_t \\right)^2.$$ \n",
    "    - **end if**\n",
    "\n",
    "    \n",
    "- **end for**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611a320-4126-482b-a6bf-8834b195a568",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a2e61",
   "metadata": {},
   "source": [
    "Following the paper's implementation, the Policy network is an MLP with two hidden layers, 64 neurons, and tanh activation function. The coefficient $c_1$ in the final objective function (critic loss + actor loss) is discarted since the policy and the value networks do not share parameters between each other. And the entropy term used to ensure enough exploration is also ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92555d0-2e64-4a47-b4b5-0812b81b867a",
   "metadata": {},
   "source": [
    "When computing the stochastic gradient ascent with an automatic differentiation software, the implementation of PPO without shared parameters between Actor and Critic networks uses the $L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_{old}, \\theta)$ objective loss function instead of the traditional objective function $L^{PG} = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [log \\left( \\pi_{\\theta} (a_t | s_t) \\right) \\Phi_t]$ from vanilla policy gradient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e95330de-395a-4ba5-bf89-33f9546b4757",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87ccd3-daab-46bb-9faa-b8624e6d1cdd",
   "metadata": {},
   "source": [
    "[1] [Proximal Policy Optimization Algorithms, Schulman et al. 2017.](https://arxiv.org/abs/1707.06347)\n",
    "\n",
    "[2] https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "[3] https://drive.google.com/file/d/1PDzn9RPvaXjJFZkGeapMHbHGiWWW20Ey/view"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
