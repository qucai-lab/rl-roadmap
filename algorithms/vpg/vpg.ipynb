{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a5eb54",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <h1> \n",
    "        <a href='https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf'> Vanilla Policy Gradient (VPG) </a> \n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502767b3-445f-4693-8c30-48ee72a0a2c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a8db2",
   "metadata": {},
   "source": [
    "Vanilla Policy Gradient (VPG) a.k.a REINFORCE is a rosetta stone of modern reinforcement learning. It provides the theoretical basis for many model-free RL algorithms based on policy gradient methods such as DDPG, TRPO, and PPO.\n",
    "\n",
    "- VPG is not an actor-critic algorithm, instead, it is a pure policy-based reinforcement learning algorithm that **uses only one neural network** to estimate the policy function $\\pi_{\\theta}$. If a baseline function $b(s_t)$ is used to reduce the variance of the policy gradient estimates, then **a second neural network can be introduced** to estimate the baseline function.\n",
    "\n",
    "- VPG has only one objective loss function: the policy gradient loss $L^{PG} = J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [log \\left( \\pi_{\\theta} (a_t | s_t) \\right) \\Phi_t]$.\n",
    "\n",
    "- VPG optimizes a `stochastic policy` $a_t \\sim \\pi_{\\theta}(a_t | s_t)$ directly using `gradient ascent` on the loss function (performance objective) $L^{PG}$ while off-policy algorithms, such as DDPG and Q-learning, use the Bellman optimality equation.\n",
    "  \n",
    "- VPG is suitable for both `continuous and discrete action spaces`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a20ec1-8c53-4772-a0db-876be2c865fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# The RL Goal in VPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e136619-0d06-4bb0-a51c-90faabef83f8",
   "metadata": {},
   "source": [
    "The goal of reinforcement learning is to maximize the expected cumulative reward (a.k.a expected return) $J(\\pi_{\\theta})$ under a parameterized policy $\\pi_{\\theta}$:\n",
    "\n",
    "$$\\text{max } J(\\pi_{\\theta}) = \\text{max } \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau) | \\pi_{\\theta_t}].$$\n",
    "\n",
    "In Policy Gradient algorithms, such as VPG, this can be achieved by directly updating/optimizing the parameters $\\theta_t$ of the parameterized Policy $\\pi_{\\theta}$ computing the `gradient ascent` of the performance objective $J(\\pi_{\\theta})$ with respect to the Policy parameters:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\theta_{t+1} &=& \\theta_t + \\alpha \\nabla_{\\theta} J(\\pi_{\\theta})\\\\\n",
    "&=& \\theta_t + \\alpha \\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau) | \\pi_{\\theta_t}].\n",
    "\\end{eqnarray}\n",
    "\n",
    "The general form of the Policy Gradient, which follows from the **Policy Gradient Theorem** (Sutton et al., 1999), is: \n",
    "\n",
    "$$ \n",
    "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\sum_{t=0}^T \\nabla_{\\theta} log \\left( \\pi_{\\theta} (a_t | s_t) \\right) \\Phi_t,\n",
    "$$\n",
    "\n",
    "where $\\Phi_t$ is any of the following functions:\n",
    "\n",
    "- $\\Phi_t = \\mathcal{R}(\\tau)$.\n",
    "- $\\Phi_t = Q^{\\pi_{\\theta}} (s_t, a_t)$.\n",
    "- $\\Phi_t = A^{\\pi_{\\theta}} (s_t, a_t)$.\n",
    "- $\\Phi_t = \\sum_{t=0}^T  \\mathcal{R} (s, a, s_{t+1})$.\n",
    "- $\\Phi_t = \\sum_{t=0}^T \\mathcal{R} (s, a, s_{t+1}) - b(s_t)$.\n",
    "\n",
    "Legend:\n",
    "\n",
    "-  $\\pi_{\\theta} (\\cdot)$ is the parameterized Policy function represented by a neural network as an expressive nonlinear function approximation.\n",
    "  \n",
    "- $\\nabla_{\\theta} J(\\pi_{\\theta})$ denotes the gradient of `policy performance` a.k.a `policy gradient`.\n",
    "\n",
    "- $\\mathcal{R}(\\tau) \\doteq \\sum_{t=0}^T r_t \\in {\\rm I\\!R} \\text{ (Finite-horizon undiscounted return)}$ is the sum of rewards over a fixed window of time steps.\n",
    "\n",
    "- $\\mathcal{R}(\\tau) \\doteq \\sum_{t=0}^{\\infty} \\gamma^t r_{t} \\in {\\rm I\\!R} \\text{ (Infinite-horizon discounted return)}$ is the sum of all rewards ever obtained.\n",
    "\n",
    "- $Q^{\\pi_{\\theta}}$ denotes the On-Policy Action-Value function.\n",
    "\n",
    "- $A^{\\pi_{\\theta}}$ denotes the Advantage function.\n",
    "\n",
    "- $b (s_t)$ denotes the baseline.\n",
    "  \n",
    "- $\\alpha$ is the learning rate hyperparameter.\n",
    "\n",
    "- $\\gamma \\in [0,1]$ is the discount factor hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1457d36-d599-4bcc-8162-74b8c3f85806",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Deriving the Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c82a3-4a54-42d6-a3fe-6a3941878760",
   "metadata": {},
   "source": [
    "Considering the infinite-horizon discounted return function $\\mathcal{R}(\\tau)$, the performance objective $J(\\pi_{\\theta})$ can be defined as the expected cumulative reward under a parameterized policy $\\pi_{\\theta}$, which is mathematically represented as:\n",
    "\n",
    "$$J(\\pi_{\\theta}) =  \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau)] = \\int_{\\tau} \\underbrace{\\mathbb{P}(\\tau | \\theta)}_{\\text{Trajectory prob.}}  \\underbrace{\\mathcal{R}(\\tau)}_{\\text{Return}}.$$\n",
    " \n",
    "Where the probability of trajectory $\\tau$ under the policy $\\pi_{\\theta}$ is given by\n",
    "\n",
    "$$\\mathbb{P}(\\tau | \\theta) = \\underbrace{\\rho_0(s_0)}_{\\text{start-state dist.}} \\prod_{t=0}^{T} \\underbrace{\\underbrace{\\mathbb{P}(s_{t+1}|s_t,a_t)}_{\\text{State transition prob.}}}_{\\text{Env. model.}} \\cdot \\underbrace{\\underbrace{\\pi_{\\theta}(a_t | s_t)}_{\\text{Action prob.}}}_{\\text{Control function.}}.$$\n",
    "\n",
    "Decomposing the integral, one has:\n",
    "\n",
    "$$\n",
    "J(\\pi_{\\theta}) = \n",
    "\\int_{S} \\Bigg( \\rho_0(s_0) \\prod_{t=0}^T \\mathbb{P}(s_{t+1}|s_t,a_t) \\Bigg) ds\n",
    "\\int_{A} \\Bigg( \\pi_{\\theta}(a_t | s_t) \\mathcal{R}(\\tau) \\Bigg) da.\n",
    "$$\n",
    "\n",
    "This expression includes the state transition probability $\\mathbb{P}(s_{t+1}|s_t, a_t)$, which represents the dynamics of the environment. In `model-based` reinforcement learning, these dynamics are **explicitly** modeled. In `model-free` reinforcement learning there is no model of the environment, i.e., no state transition probability for planning (lookahead). Instead, algorithms of this kind (such as in VPG, DDPG, TRPO, PPO, etc.) rely on sampling trajectories from the unknown environment to approximate the gradient of the policy. These trajectories are collected via trial-and-error interactions.\n",
    "\n",
    "**Why is the state transition probability used in the derivation of the policy gradient for model-free RL?** Even though the transition probability appears in the theoretical definition of the prob. of a trajectory and the performance objective $J(\\pi_{\\theta})$, the policy gradient theorem reformulates the gradient into an expectation over sampled trajectories, eliminating the need for explicit knowledge or modeling of the transition probabilities. These transition probabilities are implicitly accounted for when sampling trajectories from the environment.\n",
    "\n",
    "Legend:\n",
    "\n",
    "- $\\tau = (s_0, a_0, s_1, a_1, \\dots)$ represents a trajectory (a sequence of states and actions).\n",
    "\n",
    "- $\\pi_{\\theta}$ is the parameterized Policy represented by a neural network as the expressive nonlinear function approximator to estimate/represent the Policy function $\\pi$, i.e., the stochastic action probability distribution a.k.a control function.\n",
    "\n",
    "- $\\mathbb{P(\\tau | \\theta)}$: is the probability of getting a trajectory $\\tau$ with $T$ time steps acting according to policy $\\pi_{\\theta}$.\n",
    "\n",
    "- $\\mathbb{P}(s_{t+1}|s_t,a_t)$ denotes the state transition probability of ending up in the next state $s_{t+1}$ given the current state $s_t$ and action $a_t$.\n",
    "\n",
    "- $\\rho_0$ denotes the initial (start)-state probability distribution. The initial state $s_0$ is sampled from the probability distribution: $s_0 \\sim \\rho_0(\\cdot).$\n",
    "\n",
    "- $\\mathcal{R}(\\tau) = \\sum_{t=0}^{T} \\gamma^t r_t$ is the cumulative (discounted) reward for the trajectory over an episode.\n",
    "\n",
    "- $\\gamma \\in [0, 1]$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772616a-a8f6-41be-b95f-18c274c38cac",
   "metadata": {},
   "source": [
    "To compute the policy gradient numerically, one must first derive an analytical expression for the policy gradient in terms of the expected value and then sample trajectories through agent-environment interaction steps. The policy gradient reads:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\nabla_{\\theta} J (\\pi_{\\theta}) &=& \\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau)] \\quad (Eq. 1.1) \\\\\n",
    "&=& \\nabla_{\\theta} \\int_{\\tau} \\mathbb{P}(\\tau | \\theta) [\\mathcal{R}(\\tau)] \\quad (Eq. 1.2) \\\\\n",
    "&=& \\int_{\\tau} \\nabla_{\\theta} \\mathbb{P}(\\tau | \\theta)  [\\mathcal{R}(\\tau)] \\quad (Eq. 1.3) \\\\\n",
    "&=& \\int_{\\tau}  \\mathbb{P}(\\tau | \\pi_{\\theta}) \\Bigg[ \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\theta))  \\mathcal{R}(\\tau) \\Bigg] \\quad (Eq. 1.4) \\\\\n",
    "&=& \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\Bigg[ \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\theta))  \\mathcal{R}(\\tau) \\Bigg] \\quad (Eq. 1.5) \\\\\n",
    "&=& \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T\\nabla_{\\theta}log(\\pi_{\\theta}(a_t| s_t))\\mathcal{R}(\\tau) \\right] \\quad (Eq. 1.6)\\\\\n",
    "&\\approx& \\frac{1}{|\\mathcal{D}_k|}\\sum_{\\tau \\in \\mathcal{D}_k}\\sum_{t=0}^T\\nabla_{\\theta}log(\\pi_{\\theta}(a_t| s_t))|_{\\theta_k}\\mathcal{R}(\\tau) \\quad (Eq. 1.7).\n",
    "\\end{eqnarray}\n",
    "\n",
    "$\\quad$\n",
    "\n",
    "- Eq. 1.2 uses the definition of the expected return, noting that $\\mathbb{E} [\\cdot] = \\int \\mathbb{P}(\\cdot) [\\cdot]$.\n",
    "\n",
    "<div style=\"background-color: green; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "**Expected value (a.k.a population mean or weighted average)  of a distribution:**\n",
    "\n",
    "- Expected value for **discrete variables**:\n",
    "    $$\\mu \\doteq  \\mathbb{E}[X]  \\doteq \\langle X\\rangle = \\sum_{j=1}^{\\dim \\Omega = d} x_jP(x_j).$$\n",
    "    The expected value is not the most likely value of $X$ and may not even be a possible value of $X$, but it is bound by\n",
    "    $$X_{\\min} \\leq\\langle X\\rangle\\leq X_{\\max}.$$\n",
    "\n",
    "- Expected value for **continuous variables**:\n",
    "    $$\\langle X \\rangle = \\int_{-\\infty}^{+\\infty} x \\rho(x) dx.$$\n",
    "  \n",
    "</div>\n",
    "  \n",
    "- Eq. 1.3 uses the [Leibniz integral rule](https://en.wikipedia.org/wiki/Leibniz_integral_rule) to bring the gradient symbol under the integral sign. This is possible because the integration domain (over $\\tau$) does not depend on $\\theta$. \n",
    "  \n",
    "- Eq. 1.4 uses the log-derivative trick:\n",
    "\n",
    "  \\begin{eqnarray}\\\n",
    "  \\frac{d}{dx}ln(f(x)) &=& \\frac{1}{f(x)}\\frac{d}{dx}f(x). \\\\\n",
    "                         &\\rightarrow&  \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\theta)) = \\frac{1}{\\mathbb{P}(\\tau | \\theta)}  \\nabla_{\\theta} \\mathbb{P}(\\tau | \\theta)\\\\\n",
    "                       &\\rightarrow&  \\nabla_{\\theta} \\mathbb{P}(\\tau | \\theta) =  \\mathbb{P}(\\tau | \\theta) \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\theta))\n",
    "  \\end{eqnarray}\n",
    "\n",
    "- Eq. 1.5 is the expectation form of Eq. 1.4 using $\\mathbb{E} [\\cdot] = \\int \\mathbb{P}(\\cdot) [\\cdot]$.\n",
    "\n",
    "- Eq. 1.6 known as the `policy gradient theorem` denotes the expected value of the **gradient of the log-probability of a trajectory**, weighted by the return $\\mathcal{R}$, over all trajectories sampled i.i.d from the policy.\n",
    "\n",
    "<div style=\"background-color: green; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "**Derivation:**\n",
    "\n",
    "  \\begin{eqnarray}\n",
    "   \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\pi_{\\theta})) &=& \\nabla_{\\theta} log \\Bigg( \\rho_0(s_0) \\prod_{t=0}^{T} \\mathbb{P}(s_{t+1}|s_t,a_t) \\cdot \\pi(a_t | s_t) \\Bigg) \\\\\n",
    "   &=& \\nabla_{\\theta} \\Bigg[log (\\rho_0(s_0)) + log \\left( \\prod_{t=0}^{T} \\mathbb{P}(s_{t+1}|s_t,a_t) \\cdot \\pi(a_t | s_t) \\right) \\Bigg]\\\\\n",
    "   &=& \\nabla_{\\theta} \\Bigg[log (\\rho_0(s_0)) + \\sum_{t=0}^{T} log \\Big( \\mathbb{P}(s_{t+1}|s_t,a_t) \\cdot \\pi(a_t | s_t) \\Big) \\Bigg]\\\\\n",
    "   &=& \\nabla_{\\theta} \\Bigg[log (\\rho_0(s_0)) + \\sum_{t=0}^{T} log \\Big(\\mathbb{P}(s_{t+1}|s_t,a_t)\\Big) + log \\Big(\\pi(a_t | s_t)\\Big) \\Bigg]\\\\\n",
    "   &=& \\Bigg[\\cancel{\\nabla_{\\theta} log (\\rho_0(s_0))} + \\sum_{t=0}^{T} \\cancel{\\nabla_{\\theta} log \\Big(\\mathbb{P}(s_{t+1}|s_t,a_t)\\Big)} + \\nabla_{\\theta} log \\Big(\\pi(a_t | s_t)\\Big) \\Bigg]\\\\\n",
    "   &=& \\sum_{t=0}^T\\nabla_{\\theta}log(\\pi_{\\theta}(a_t| s_t))\n",
    "  \\end{eqnarray}\n",
    "</div>\n",
    "\n",
    "- Eq. 1.7 computes the empirical average (approximation) of Eq. 1.6. It is an unbiased estimator (sample mean) of the expectation $\\mathbb{E}_{\\tau \\sim \\pi_{\\theta_k}} [\\cdot]$ since trajectories are sampled i.i.d from the policy. As the number of trajectories $|\\mathcal{D}_k| \\rightarrow \\infty$, Eq. 1.7 converges to Eq. 1.6 due to the Law of Large Numbers.\n",
    " \n",
    "Legend:\n",
    "\n",
    "- $\\mathcal{D}_k$ denotes the set with a number $|\\mathcal{D}_k|$ of trajectories sampled i.i.d from the Policy $\\pi_k$ in the $k$-th iteration.\n",
    "- i.i.d means independent and identically distributed.\n",
    "    - Independent: the outcome of one random variable does not affect the outcomes of the others.\n",
    "    - Identically Distributed: all random variables in the collection follow the same probability distribution (e.g., same mean and variance). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdebf77-c27c-49a4-9123-543e04d3f9ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Reducing Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12587c75-69ab-48db-a224-6e2f567e1aa2",
   "metadata": {},
   "source": [
    "One way to reduce the variance of the policy gradient estimates is to add a baseline $b$ (average return) to the policy gradient formula.\n",
    " \n",
    "For any choice of baseline $b(s_t)$ that depends only on the state $s_t$, the gradient estimator is unbiased, i.e., introducing a baseline into the policy gradient formula does not change the expected value of the policy gradient sample estimate. This is a consequence of the Expected Grad-Log-Prob (EGLP) lemma:\n",
    "\n",
    "$$ \\mathbb{E}_{x \\sim P_{\\theta}}[\\nabla_{\\theta} log P_{\\theta} (x)] = 0,$$\n",
    "\n",
    "such that\n",
    "\n",
    "$$ \\mathbb{E}_{a_t \\sim \\pi_{\\theta}}[\\nabla_{\\theta} log \\pi_{\\theta}(a_t|s_t) b(s_t)] = 0,$$\n",
    "\n",
    "and \n",
    "\n",
    "$$ \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\Bigg[ \\sum_{t=0}^T \\nabla_{\\theta} log \\pi_{\\theta}(a_t|s_t) \\bigg( \\sum_{t'=t}^T \\mathcal{R}(s_{t'}, a_{t'}, s_{t'+1}) - b(s_t) \\bigg) \\Bigg],$$\n",
    "\n",
    "A near-optimal choice for the baseline in VPG is the `on-policy State Value` function $b (s_t) = V^{\\pi_{\\theta}}(s_t)$. The difference between the `on-policy Action-Value` function $Q(\\cdot)$ and the baseline is the `Advantage function`:\n",
    "\n",
    "$$A^{\\pi_{\\theta}}(s_t, a_t) = Q^{\\pi_{\\theta}}(s_t, a_t) - V^{\\pi_{\\theta}}(s_t) \\in {\\rm I\\!R}.$$\n",
    "\n",
    "The baseline $V^{\\pi_{\\theta}}(s_t)$ is often approximated by a neural network $V_{\\phi}(s_t)$ since it cannot be computed exactly. \n",
    "\n",
    "To reduce variance even further one can introduce discount factors. However, if the time horizon is too long, i.e, if there are too many time steps in one episode, VPG will likely not work well. Therefore, the discount factor should be think off as a variance reduction parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30212f-6532-495c-88f5-9bed3c6dfcb1",
   "metadata": {},
   "source": [
    "Recall the definitions of the value functions as the expected cumulative future reward a.k.a expected (average) return an agent can get if starting from state $s_0$ and acting according to actions $a$ sampled from a parameterized **stochastic (if  VPG)** policy $\\pi_{\\theta}$: \n",
    "\n",
    "- Parameterized On-Policy Action-Value Function:\n",
    "\n",
    "$$Q^{\\pi_{\\theta}}(s, a): S \\rightarrow {\\rm I\\!R}.$$\n",
    "$$Q^{\\pi_{\\theta}}(s, a) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau) | s_0 = s, a_0 = a] = \\frac{1}{N}\\sum_{n=1}^N \\mathcal{R}_t^n.$$\n",
    "\n",
    "- Parameterized On-Policy State-Value Function:\n",
    "\n",
    "$$V^{\\pi}(s): S \\rightarrow {\\rm I\\!R}.$$\n",
    "$$V^{\\pi}(s) = \\mathbb{E}_{\\tau \\sim \\pi}[\\mathcal{R}(\\tau) | s_0 = s].$$\n",
    "$$V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi}[Q^{\\pi}(s, a)].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17878cb-0cc7-46c9-9e5a-36d2e5a35e80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df02c54-d70b-41d2-8f23-f94a6db0f3c3",
   "metadata": {},
   "source": [
    "The following algorithm uses the advantage-based expression for the policy gradient, instead of the finite-horizon undiscounted return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242adc76-938a-4aa3-98a5-5fc8a3cad57c",
   "metadata": {},
   "source": [
    "---\n",
    "**Algorithm (Pseudocode): Vanilla Policy Gradient (adapted from Open AI)**\n",
    "\n",
    "---\n",
    "\n",
    "- Input: initialize policy parameters $\\theta_0$, and baseline parameters $\\phi_0$ of the state value function $V_{\\phi}$.\n",
    "\n",
    "- **for** episode $k = 0, 1, 2, \\dots$ do:\n",
    "    - Collect a set of trajectories $\\mathcal{D}_k\\doteq\\{\\tau_i\\} = (s_0, a_0, r_0, \\cdots , s_T, a_{T}, r_T)$ by executing the current policy $\\pi_k = \\pi(\\theta_k)$ in the environment for $T$ time steps.\n",
    "    - **for** each trajectory, compute: \n",
    "        - the reward-to-go $\\hat{\\mathcal{R}}_t = \\sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1})$, and\n",
    "        - the advantage estimate $\\hat{A}_t$ (using any advantage estimation method) based on the current on-policy state value function $V_{\\phi_k}$ as the baseline to reduce sample variance in the gradient estimate: $$\\hat{A}_t = Q^{\\pi_{\\theta_k}}(s_t, a_t) - V_{\\phi_k}(s_t) \\in {\\rm I\\!R}.$$\n",
    "    - **end for**.\n",
    "    - Estimate the Policy gradient as: $$\\nabla_{\\theta} J (\\pi_{\\theta_k}) \\equiv \\hat{g}_k = \\frac{1}{|\\mathcal{D}_k|}\\sum_{\\tau \\in \\mathcal{D}_k}\\sum_{t=0}^T\\nabla_{\\theta}log\\pi_{\\theta}(a_t| s_t)|_{\\theta_k}\\hat{A}_t.$$\n",
    "    - Update the Policy either using standard gradient ascent or via another `gradient ascent` algorithm (such as Adam): $$\\theta_{k+1}= \\theta_k + a_k \\hat{g}_k,$$  where $a_k$ is the learning rate in the $k$-th time step.\n",
    "    - Re-fit (learn) the baseline (state value function) by regression on mean-squared error, via some `gradient descent` algorithm, by minimizing $(V_{\\phi}(s_t)-\\hat{\\mathcal{R}}_t)^2$ summed over all trajectories and time steps: $$\\phi_{k+1} = \\underset{\\phi}{\\operatorname{arg\\,min}} \\ \\frac{1}{|\\mathcal{D}_k|T}\\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^T \\left(V_{\\phi}(s_t) - \\hat{\\mathcal{R}}_t \\right)^2 .$$\n",
    "        \n",
    "    - **end for.**\n",
    "- **end for.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecfbc2d-90e2-4c0a-b16d-7adf750f48d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73e3e4-732b-4e25-a65c-f8c73e962790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f8aceb9-fda9-4472-8593-e8493cdf883e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4672816-c494-4caa-8eea-8e06fea54f31",
   "metadata": {},
   "source": [
    "[1] [Policy Gradient Methods for Reinforcement Learning with Function Approximation, Sutton et al. 2000.](https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)\n",
    "\n",
    "[2] [Optimizing Expectations: From Deep Reinforcement Learning to Stochastic Computation Graphs, Schulman 2016(a)](http://joschu.net/docs/thesis.pdf)\n",
    "\n",
    "[3] [Benchmarking Deep Reinforcement Learning for Continuous Control, Duan et al. 2016](https://arxiv.org/abs/1604.06778)\n",
    "\n",
    "[4] [High Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al. 2016(b)](https://arxiv.org/abs/1506.02438)\n",
    "\n",
    "[2] https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html \n",
    "\n",
    "[3] https://spinningup.openai.com/en/latest/algorithms/vpg.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
