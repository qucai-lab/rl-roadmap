{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a5eb54",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <h1> \n",
    "        <a href='#'> Vanilla Policy Gradient (VPG) </a> \n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502767b3-445f-4693-8c30-48ee72a0a2c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a8db2",
   "metadata": {},
   "source": [
    "Vanilla Policy Gradient (VPG) a.k.a REINFORCE is a rosetta stone for policy gradient methods and enhanced `on-policy` algorithms (such as TRPO and PPO). VPG learns the Policy function directly, while less stable off-policy algorithms, such as DDPG and Q-learning, use the Bellman optimality equation. VPG optimizes a `stochastic policy` and it is suitable for both `continuous and discrete action spaces`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1457d36-d599-4bcc-8162-74b8c3f85806",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Deriving the Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e136619-0d06-4bb0-a51c-90faabef83f8",
   "metadata": {},
   "source": [
    "The goal of reinforcement learning is to maximize the expected cumulative reward (a.k.a expected return) under policy $\\pi_{\\theta}$:\n",
    "\n",
    "$$\\text{max } J(\\pi_{\\theta}) = \\text{max } \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau) | \\pi_{\\theta_t}].$$\n",
    "\n",
    "In Policy Gradient Algorithms, this can be achieved by directly updating/optimizing the parameters $\\theta_t$ of the parameterized Policy $\\pi_{\\theta}$ computing the `gradient ascent` of the performance objective $J(\\pi_{\\theta})$ with respect to the Policy parameters $\\theta_t$:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\theta_{t+1} &=& \\theta_t + \\alpha \\nabla_{\\theta} J(\\pi_{\\theta})|_{\\theta_t}\\\\\n",
    "&=& \\theta_t + \\alpha \\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau) | \\pi_{\\theta_t}].\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "-  $\\pi_{\\theta}$ is the parameterized Policy function represented by a neural network as an expressive nonlinear function approximation.\n",
    "\n",
    "- $\\alpha$ is the learning rate.\n",
    "  \n",
    "- $\\nabla_{\\theta} J(\\pi_{\\theta})$ denotes the gradient of `policy performance` a.k.a `policy gradient`.\n",
    "\n",
    "- $\\mathcal{R}(\\tau) \\doteq \\sum_{t=0}^T r_t \\in {\\rm I\\!R} \\text{ (Finite-horizon undiscounted return)}$ is the sum of rewards over a fixed window of time steps.\n",
    "\n",
    "- $\\mathcal{R}(\\tau) \\doteq \\sum_{t=0}^{\\infty} \\gamma^t r_{t} \\in {\\rm I\\!R} \\text{ (Infinite-horizon discounted return)}$ is the sum of all rewards ever obtained.\n",
    "\n",
    "- $\\gamma \\in [0,1]$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c82a3-4a54-42d6-a3fe-6a3941878760",
   "metadata": {},
   "source": [
    "Considering the infinite-horizon discounted return function, the performance objective $J(\\pi_{\\theta})$ is defined as the expected cumulative reward under a parameterized policy $\\pi_{\\theta}$, which is mathematically represented as:\n",
    "\n",
    "$$J(\\pi_{\\theta}) =  \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau)] = \\int_{\\tau} \\underbrace{\\mathbb{P}(\\tau | \\pi_{\\theta})}_{\\text{Trajectory prob.}}  \\underbrace{\\mathcal{R}(\\tau)}_{\\text{Return}},$$\n",
    "\n",
    "with the the probability of trajectory $\\tau$ under the policy $\\pi_{\\theta}$ given by\n",
    "\n",
    "$$\\mathbb{P}(\\tau | \\pi_{\\theta}) = \\underbrace{\\rho_0(s_0)}_{\\text{start-state dist.}} \\prod_{t=0}^{T} \\underbrace{\\underbrace{\\mathbb{P}(s_{t+1}|s_t,a_t)}_{\\text{State transition prob.}}}_{\\text{Env. model.}} \\cdot \\underbrace{\\underbrace{\\pi(a_t | s_t)}_{\\text{Action prob.}}}_{\\text{Control function.}}.$$\n",
    "\n",
    "\n",
    "Legend:\n",
    "\n",
    "- $\\tau = (s_0, a_0, s_1, a_1, \\dots)$ represents a trajectory (a sequence of states and actions).\n",
    "\n",
    "-  $\\pi_{\\theta}$ is the parameterized Policy function represented by a neural network as an expressive nonlinear function approximation.\n",
    "\n",
    "- $\\mathbb{P(\\tau | \\pi_{\\theta})}$: is the probability of getting a trajectory $\\tau$ with $T$ time steps acting according to policy $\\pi_{\\theta}$.\n",
    "\n",
    "- $\\mathbb{P}(s_{t+1}|s_t,a_t)$ denotes the state transition probability of ending up in the next state $s_{t+1}$ given the current state $s_t$ and action $a_t$.\n",
    "\n",
    "- $\\rho_0$ denotes the initial (start)-state probability distribution. The initial state $s_0$ is sampled from the probability distribution: $s_0 \\sim \\rho_0(\\cdot).$\n",
    "\n",
    "- $\\mathcal{R}(\\tau) = \\sum_{t=0}^{T} \\gamma^t r_t$ is the cumulative (discounted) reward for the trajectory over an episode.\n",
    "\n",
    "- $\\gamma \\in [0, 1]$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772616a-a8f6-41be-b95f-18c274c38cac",
   "metadata": {},
   "source": [
    "To compute the policy gradient numerically, one must first derive an analytical expression for the policy gradient in terms of the expected value and then sample trajectories through agent-environment interaction steps. The policy gradient reads:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\nabla_{\\theta} J (\\pi_{\\theta}) &=& \\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau)] \\quad (Eq. 1.1) \\\\\n",
    "&=& \\nabla_{\\theta} \\int_{\\tau} \\mathbb{P}(\\tau | \\pi_{\\theta}) [\\mathcal{R}(\\tau)] \\quad (Eq. 1.2) \\\\\n",
    "&=& \\int_{\\tau} \\nabla_{\\theta} \\mathbb{P}(\\tau | \\pi_{\\theta})  [\\mathcal{R}(\\tau)] \\quad (Eq. 1.3) \\\\\n",
    "&=& \\int_{\\tau}  \\mathbb{P}(\\tau | \\pi_{\\theta}) \\Bigg[ \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\pi_{\\theta}))  \\mathcal{R}(\\tau) \\Bigg] \\quad (Eq. 1.4) \\\\\n",
    "&=& \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\Bigg[ \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\pi_{\\theta}))  \\mathcal{R}(\\tau) \\Bigg] \\quad (Eq. 1.5) \\\\\n",
    "&=& \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T\\nabla_{\\theta}log(\\pi_{\\theta}(a_t| s_t))\\mathcal{R}(\\tau) \\right] \\quad (Eq. 1.6)\\\\\n",
    "&\\approx& \\frac{1}{|\\mathcal{D}_k|}\\sum_{\\tau \\in \\mathcal{D}_k}\\sum_{t=0}^T\\nabla_{\\theta}log(\\pi_{\\theta}(a_t| s_t))|_{\\theta_k}\\mathcal{R}(\\tau) \\quad (Eq. 1.7).\n",
    "\\end{eqnarray}\n",
    "\n",
    "$\\quad$\n",
    "\n",
    "- Eq. 1.2 uses the definition of the expected return, noting that $\\mathbb{E} [\\cdot] = \\int \\mathbb{P}(\\cdot) [\\cdot]$.\n",
    "\n",
    "<div style=\"background-color: yellow; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "**Expected value (a.k.a population mean or weighted average)  of a distribution:**\n",
    "\n",
    "- Expected value for **discrete variables**:\n",
    "    $$\\mu \\doteq  \\mathbb{E}[X]  \\doteq \\langle X\\rangle = \\sum_{j=1}^{\\dim \\Omega = d} x_jP(x_j).$$\n",
    "    The expected value is not the most likely value of $X$ and may not even be a possible value of $X$, but it is bound by\n",
    "    $$X_{\\min} \\leq\\langle X\\rangle\\leq X_{\\max}.$$\n",
    "\n",
    "- Expected value for **continuous variables**:\n",
    "    $$\\langle X \\rangle = \\int_{-\\infty}^{+\\infty} x \\rho(x) dx.$$\n",
    "  \n",
    "</div>\n",
    "  \n",
    "- Eq. 1.3 uses the [Leibniz integral rule](https://en.wikipedia.org/wiki/Leibniz_integral_rule) to bring the gradient symbol under the integral sign. This is possible because the integration domain (over $\\tau$) does not depend on $\\theta$. \n",
    "  \n",
    "- Eq. 1.4 uses the log-derivative trick:\n",
    "\n",
    "  \\begin{eqnarray}\\\n",
    "  \\frac{d}{dx}ln(f(x)) &=& \\frac{1}{f(x)}\\frac{d}{dx}f(x). \\\\\n",
    "                         &\\rightarrow&  \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\pi_{\\theta})) = \\frac{1}{\\mathbb{P}(\\tau | \\pi_{\\theta})}  \\nabla_{\\theta} \\mathbb{P}(\\tau | \\pi_{\\theta})\\\\\n",
    "                       &\\rightarrow&  \\nabla_{\\theta} \\mathbb{P}(\\tau | \\pi_{\\theta}) =  \\mathbb{P}(\\tau | \\pi_{\\theta}) \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\pi_{\\theta}))\n",
    "  \\end{eqnarray}\n",
    "\n",
    "- Eq. 1.5 is the expectation form of Eq. 1.4 using $\\mathbb{E} [\\cdot] = \\int \\mathbb{P}(\\cdot) [\\cdot]$.\n",
    "\n",
    "- Eq. 1.6 computes the expected value of the **gradient of the log-probability of a trajectory**, weighted by the return $\\mathcal{R}$, over all trajectories sampled i.i.d from the policy.\n",
    "\n",
    "<div style=\"background-color: yellow; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "**Derivation:**\n",
    "\n",
    "  \\begin{eqnarray}\n",
    "   \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\pi_{\\theta})) &=& \\nabla_{\\theta} log \\Bigg( \\rho_0(s_0) \\prod_{t=0}^{T} \\mathbb{P}(s_{t+1}|s_t,a_t) \\cdot \\pi(a_t | s_t) \\Bigg) \\\\\n",
    "   &=& \\nabla_{\\theta} \\Bigg[log (\\rho_0(s_0)) + log \\left( \\prod_{t=0}^{T} \\mathbb{P}(s_{t+1}|s_t,a_t) \\cdot \\pi(a_t | s_t) \\right) \\Bigg]\\\\\n",
    "   &=& \\nabla_{\\theta} \\Bigg[log (\\rho_0(s_0)) + \\sum_{t=0}^{T} log \\Big( \\mathbb{P}(s_{t+1}|s_t,a_t) \\cdot \\pi(a_t | s_t) \\Big) \\Bigg]\\\\\n",
    "   &=& \\nabla_{\\theta} \\Bigg[log (\\rho_0(s_0)) + \\sum_{t=0}^{T} log \\Big(\\mathbb{P}(s_{t+1}|s_t,a_t)\\Big) + log \\Big(\\pi(a_t | s_t)\\Big) \\Bigg]\\\\\n",
    "   &=& \\Bigg[\\cancel{\\nabla_{\\theta} log (\\rho_0(s_0))} + \\sum_{t=0}^{T} \\cancel{\\nabla_{\\theta} log \\Big(\\mathbb{P}(s_{t+1}|s_t,a_t)\\Big)} + \\nabla_{\\theta} log \\Big(\\pi(a_t | s_t)\\Big) \\Bigg]\\\\\n",
    "   &=& \\sum_{t=0}^T\\nabla_{\\theta}log(\\pi_{\\theta}(a_t| s_t))\n",
    "  \\end{eqnarray}\n",
    "</div>\n",
    "\n",
    "- Eq. 1.7 computes the empirical average (approximation) of Eq. 1.6. It is an unbiased estimator (sample mean) of the expectation $\\mathbb{E}_{\\tau \\sim \\pi_{\\theta_k}} [\\cdot]$ since trajectories are sampled i.i.d from the policy. As the number of trajectories $|\\mathcal{D}_k| \\rightarrow \\infty$, Eq. 1.7 converges to Eq. 1.6 due to the Law of Large Numbers.\n",
    " \n",
    "Legend:\n",
    "\n",
    "- $\\mathcal{D}_k$ denotes the set with a number $|\\mathcal{D}_k|$ of trajectories sampled i.i.d from the Policy $\\pi_k$ in the $k$-th iteration.\n",
    "- i.i.d means independent and identically distributed.\n",
    "    - Independent: the outcome of one random variable does not affect the outcomes of the others.\n",
    "    - Identically Distributed: all random variables in the collection follow the same probability distribution (e.g., same mean and variance). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdebf77-c27c-49a4-9123-543e04d3f9ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reducing Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12587c75-69ab-48db-a224-6e2f567e1aa2",
   "metadata": {},
   "source": [
    "Recall the definitions of the value functions: \n",
    "\n",
    "- On-Policy Value Function:\n",
    "\n",
    "$$V^{\\pi_{\\theta}}(s_t) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau) | s_0 = s_t].$$\n",
    "$$ V^{\\pi_{\\theta}}(s_t) = \\mathbb{E}_{a_t \\sim \\pi_{\\theta}(s_t)}[Q^{\\pi}(s_t, a_t)].$$\n",
    "\n",
    "- On-Policy Action-Value Function:\n",
    "\n",
    "$$Q^{\\pi_{\\theta}}(s_t, a_t): S \\rightarrow {\\rm I\\!R}.$$\n",
    "$$Q^{\\pi_{\\theta}}(s_t, a_t) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau) | s_0 = s_t, a_0 = a_t] = \\frac{1}{N}\\sum_{n=1}^N \\mathcal{R}_t^n.$$\n",
    "\n",
    "One way to reduce variance is to add a baseline to the VPG. A near-optimal choice for the baseline in the VPG is the expected return given by the state value function $V^{\\pi_{\\theta}}(s_t)$. For any choice of baseline, the gradient estimator is unbiased. The difference between the expected return and the baseline is the advantage function:\n",
    "\n",
    "$$A^{\\pi_{\\theta}}(s_t, a_t) = Q^{\\pi_{\\theta}}(s_t, a_t) - V^{\\pi_{\\theta}}(s_t) \\in {\\rm I\\!R}.$$\n",
    "\n",
    "To reduce variance even further one can introduce discount factors. However, if the time horizon is too long, i.e, if there are too many time steps in one episode, VPG will likely not work well. Therefore, the discount factor should be think off as a variance reduction parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17878cb-0cc7-46c9-9e5a-36d2e5a35e80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242adc76-938a-4aa3-98a5-5fc8a3cad57c",
   "metadata": {},
   "source": [
    "---\n",
    "**Algorithm (Pseudocode): Vanilla Policy Gradient (adapted from Open AI)**\n",
    "\n",
    "---\n",
    "\n",
    "- Input: initialize policy parameters $\\theta_0$, and baseline parameters $\\phi_0$ of the state value function $V_{\\phi}$.\n",
    "\n",
    "- for $k= 0, 1, 2, \\dots$ do:\n",
    "    - Collect a set of trajectories $\\mathcal{H}_t \\doteq \\mathcal{D}_k\\doteq\\{\\tau_i\\} = (s_0, a_0, r_0, \\cdots , s_N, a_{N}, r_N)$ by executing the current policy $\\pi_k = \\pi(\\theta_k)$ in the environment.\n",
    "    - For each trajectory, compute: \n",
    "        - The reward-to-go $\\hat{\\mathcal{R}}_t = \\sum_{t'=t}^T R(s_t', a_t', s_{t'+1})$;\n",
    "        - The advantage estimates $\\hat{A}_1, \\dots, \\hat{A}_T$ (using any advantage estimation method) based on the current  on-policy state value function $V_{\\phi_k}$ used as the baseline: $$\\hat{A}_t = Q^{\\pi_{\\theta}}(s_t, a_t) - V^{\\pi_{\\theta}}(s_t) \\in {\\rm I\\!R}.$$ \n",
    "    - Estimate the Policy gradient as:\n",
    "$$\\nabla_{\\theta} J (\\pi_{\\theta}) \\equiv \\hat{g}_k = \\frac{1}{|\\mathcal{D}_k|}\\sum_{\\tau \\in \\mathcal{D}_k}\\sum_{t=0}^T\\nabla_{\\theta}log\\pi_{\\theta}(a_t| s_t)|_{\\theta_k}\\hat{A}_t.$$\n",
    "    - Update the Policy either using standard gradient ascent or via another `gradient ascent` algorithm (such as Adam): $$\\theta_{k+1}= \\theta_k + a_k \\hat{g}_k,$$  where $a_k$ is the learning rate in the $k$-th time step.\n",
    "    - Re-fit the baseline (value function) by regression on mean-squared error, via some `gradient descent` algorithm, by minimizing $(V_{\\phi}(s_t)-\\hat{\\mathcal{R}}_t)^2$ summed over all trajectories and time steps: $$\\phi_{k+1} = \\underset{\\phi}{\\operatorname{arg\\,min}} \\ \\frac{1}{|\\mathcal{D}_k|T}\\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^T \\left(V_{\\phi}(s_t) - \\hat{\\mathcal{R}}_t \\right)^2 .$$ \n",
    "\n",
    "    \n",
    "- end for.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecfbc2d-90e2-4c0a-b16d-7adf750f48d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73e3e4-732b-4e25-a65c-f8c73e962790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f8aceb9-fda9-4472-8593-e8493cdf883e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4672816-c494-4caa-8eea-8e06fea54f31",
   "metadata": {},
   "source": [
    "[1] https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html \n",
    "\n",
    "[2] https://spinningup.openai.com/en/latest/algorithms/vpg.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
