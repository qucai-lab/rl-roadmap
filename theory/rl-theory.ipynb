{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D89RyN_OqFul"
   },
   "source": [
    "<div align=\"center\">\n",
    "  <h1><b> RL Theory </b></h1>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<b>Author: <a target=\"_blank\" href=\"https://github.com/camponogaraviera\">Lucas Camponogara Viera</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AJHSZ377WcHF",
    "outputId": "af639a87-803e-4688-b72d-21e90de8bd4a"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(30000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 30 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IkZt2tEPWcHH"
   },
   "outputs": [],
   "source": [
    "#!jupyter nbconvert --to pdf supplementary.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIv5GN0jqgMG"
   },
   "source": [
    "---\n",
    "There is an assortment of extensive resources out there, and this notebook has no aspiration to be a surrogate. The recommended textbook for reinforcement learning is c.f. Ref. \\[[1](#2)], and for deep learning is c.f. Ref. \\[[2](#2)].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "- **[Math Notation](#math)**\n",
    "- **[The RL Framework/Problem](#rl)**\n",
    "- **[Glossary](#gloss)**\n",
    "    - Learning frameworks\n",
    "    - Markov Decision Process (MDP)\n",
    "    - Markov Chain\n",
    "    - Agent, Environment and Model\n",
    "    - Agent types\n",
    "    - Environment types\n",
    "    - State space and Observation\n",
    "    - Action space\n",
    "    - State transition probability distribution\n",
    "    - History/Trajectory/Episode/Rollout\n",
    "    - Horizon and Experience\n",
    "    - Reward signal, Reward function and Return\n",
    "    - Policy\n",
    "    - Value Functions\n",
    "    - Bellman Equations\n",
    "    - Advantage Function\n",
    "    - Exploitation vs Exploration: the RL tradeoff\n",
    "    - $\\epsilon$-greedy policy\n",
    "    - Importance sampling\n",
    "    - Bootstrapping\n",
    "    - Deadly triad issue        \n",
    "- **[Taxonomy (extended)](#tax)**\n",
    "    - Model-based RL\n",
    "    - Model-free RL\n",
    "    - Dynamic programming methods\n",
    "        - Policy Iteration\n",
    "        - Value Iteration\n",
    "            - Q-learning\n",
    "        - Modified Policy Iteration\n",
    "    - Policy optimization methods\n",
    "        - Derivative-free Optimization\n",
    "        - Policy Gradient\n",
    "    - Deep RL\n",
    "        - Deep Q-Network (DQN)\n",
    "        - Actor-Critic Methods\n",
    "- **[Open Challenges in RL](#open)**\n",
    "    - Sparse rewards\n",
    "    - Alignment problem\n",
    "- **[Avoiding memory bottlenecks in deep neural networks](#memory)**\n",
    "- **[Policy Optimization Algorithms](#algo)**\n",
    "    - Derivative-free Optimization Algorithms\n",
    "        - Cross Entropy Method (CEM)\n",
    "    - Policy Gradient Algorithms\n",
    "    - Actor-Critic Algorithms\n",
    "- **[References](#References)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "_GDi1bB97XWW",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Setup &nbsp; <a href=\"#\"><img valign=\"middle\" height=\"45px\" src=\"https://img.icons8.com/jupyter\" width=\"45\" hspace=\"0px\" vspace=\"0px\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "icHt4pRi4kZX"
   },
   "source": [
    "ðŸª¨ LaTeX config.\n",
    "\n",
    "[//]: # (Math declarations:)\n",
    "\n",
    "$\\DeclareMathOperator{\\Tr}{Tr}$\n",
    "$\\DeclareMathOperator{\\HS}{HS}$\n",
    "$\\DeclareMathOperator{\\T}{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667603896454,
     "user": {
      "displayName": "Lucas Camponogara Viera",
      "userId": "14322290658374940800"
     },
     "user_tz": -480
    },
    "hidden": true,
    "id": "q90CdXRouB-s",
    "outputId": "7474a8ad-8969-401a-be2d-634c2f95443b"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// JavaScript extension for automatic equation numbering.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber:\"AMS\" } } });\n",
    "// JavaScript extension for automatic equation numbering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "vAfth5BJkhRE",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Math notation<a name=\"math\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "NSd6mMSGF2LB"
   },
   "source": [
    "| Meaning                          | LaTeX                | symbol\n",
    "| :---:                            | :---:                | :---: \n",
    "| Is equal to                      | `=`                  | $=$ \n",
    "| Is not equal to                  | `\\ne`                | $\\ne$ \n",
    "| Is identical/congruent to        | `\\equiv`             | $\\equiv$ \n",
    "| Is defined as                    | `:=`                 | $:=$ \n",
    "| Is dot equivalent to             | `\\doteq`             | $\\doteq$\n",
    "| Is approximately equal to        | `\\approx`            | $\\approx$ \n",
    "| is similar to                    | `\\sim`               | $\\sim$ \n",
    "| Is proportional to               | `\\propto`            | $\\propto$\n",
    "| Is less than or equal to         | `\\le`                | $\\le$\n",
    "| Is greater than or equal to      | `geq`                | $\\ge$\n",
    "| Therefore                        | `\\therefore `        | $\\therefore$ \n",
    "| $p$ is equivalent to/implies $q$ | `p\\Leftrightarrow q`    | $p\\Leftrightarrow q$ \n",
    "| Plus or minus                    | `\\pm`                | $\\pm$\n",
    "| Is an element of                 | `\\in`                | $\\in$\n",
    "| Is not an element of             | `\\notin`             | $\\notin$\n",
    "| Is a subset of                   | `\\subseteq`          | $\\subseteq$\n",
    "| Is a proper subset of            | `\\subset`            | $\\subset$\n",
    "| Set intersection                 | `\\cap`               | $\\cap$\n",
    "| Set Union                        | `\\cup`               | $\\cup$\n",
    "| The set of all $x$ such that     | `\\{x: \\cdots\\}`      | $\\{x: \\cdots\\}$\n",
    "| The empty set                    | `\\varnothing`        | $\\varnothing$\n",
    "| The null element                 | `\\mathbb{O}`         | $\\mathbb{O}$\n",
    "| Exponential                      | `\\exp`               | $\\exp$\n",
    "| Product over $j$                 | `\\prod_{j}`          | $\\prod_{j}$\n",
    "| Summation over $j$               | `\\sum_{j}`           | $\\sum_{j}$\n",
    "| Limit as $N$ approaches to $a$   | `lim_{N\\rightarrow a}` | $lim_{N\\rightarrow a}$\n",
    "| Random variable                  | `x_j`                |$x_j$ \n",
    "| Number of outcomes of $x_j$      | `N_j`                |$N_j$ \n",
    "| Number of measurements           | `N`                  |$N$ \n",
    "| Probability of $x_j$           | `\\Pr(x_{j})`         | $Pr(x_{j})$\n",
    "| Probability distribution         | `\\{p_j\\}`            | $\\{p_j\\}$\n",
    "| Mean value (average)             | `\\bar{x}`            | $\\bar{x}$\n",
    "| Expected value                   | `\\langle x \\rangle`  | $\\langle x \\rangle$\n",
    "| Conjugate/Hermitian transpose    | `\\dagger`            | $\\dagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "v-Vthkrgh36f",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. The RL Framework/Problem <a name=\"rl\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "0IL3Isx7h36f"
   },
   "source": [
    "Main reference for this section: [John Schulman's PhD Thesis](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-217.html), and [David Silver's RL course](https://www.davidsilver.uk/teaching/), and [Open AI](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![MDP](../assets/mdp.png)\n",
    "\n",
    "Image showing an Agent-Environment feedback loop in the framework of a Markov Decision Process (MDP).\n",
    "\n",
    "Â© Image Credits: [Open AI](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "aYtxmmd5h36g"
   },
   "source": [
    "Reinforcement learning (RL) is a general-purpose framework for artificial intelligence and a paradigm of machine learning concerned with making a sequence of optimal actions (a.k.a control or decision making) through a trial-and-error interaction (learning process) between an agent and a previously unknown environment. This interaction is formally described in the framework of `Markov Decision Processes (MDP)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "e1QId-DPh36g"
   },
   "source": [
    "There are two fundamental problem setups (frameworks) in tabular rasa sequential decision making.\n",
    "\n",
    "- Model-free RL a.k.a The Reinforcement Learning setup (framework):\n",
    "  - The environment is initially unknown.\n",
    "  - The agent learns solely via interaction with the environment.\n",
    "  - The agent improves its Policy function.\n",
    "  - There is no transition probability distribution (function) giving the conditional probability of the next state and associated reward signal given the current state-action pair.\n",
    "  \n",
    "\n",
    "- Model-based RL a.k.a The Planning setup (framework):\n",
    "  - A model of the environment is known (the agent knows all the rules of the game, the differential equations of motion, etc.).\n",
    "  - The agent performs computations with the model (without any external interaction).\n",
    "  - The agent improves its Policy function. Known as deliberation, reasoning, introspection, pondering, thought, and search.\n",
    "  - There is a transition probability distribution (function) giving the conditional probability of the next state and associated reward signal given the current state-action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "SSaNrSMLh36h"
   },
   "source": [
    "In the RL setting, the environment gives a state $s_t$ and a reward signal $r_t$ to the agent who performs an action $a_t$ at time step $t$ according to some Policy function $\\pi(\\theta)$. This process repeats in a feedback loop.\n",
    "\n",
    "- At each time step, the agent:\n",
    "  - Executes action $a_t$.\n",
    "  - Receives observation $O_t = s_t$.\n",
    "  - Receives scalar feedback reward signal $r_t$.\n",
    "\n",
    "\n",
    "- The environment:\n",
    "  - Receives action $a_t$.\n",
    "  - Emits observation $O_{t+1} = s_{t+1}$.\n",
    "  - Emits scalar reward signal $r_{t+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.1 RL Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "usJrtkKqh36h"
   },
   "source": [
    "**Reward Hypothesis:** `all goals can be described by the maximization of the expected cumulative reward, regardless of the return function (infinite-horizon discounted, or finite-horizon undiscounted) or the Policy (deterministic, or stochastic).`\n",
    "\n",
    "**The agent's goal (a.k.a objective) in RL** is to select a parameterized policy $\\pi_{\\theta}$ that maximizes the expected return a.k.a expected cumulative reward $J(\\pi_{\\theta}) = E[R(Ï„)]$ under said policy over an episode (a.k.a trajectory or rollout). This is achieved by optimizing the parameters $\\theta$ of the parameterized policy $\\pi_{\\theta}$ (e.g., a neural network). The optimization typically involves computing the gradient of the expected return $\\nabla_\\theta J(\\pi_\\theta)$ and using it to improve the policy. The expected return $J(\\pi_{\\theta})$ is also known as performance objective.\n",
    "\n",
    "The goal can be represented by the following equation:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{max } \\eta(\\pi)&=&\\text{max } J(\\pi_{\\theta}) \\\\\n",
    "&=& \\text{max } \\mathbb{E}[\\mathcal{R}(\\tau) | \\pi_{\\theta}] \\\\\n",
    "&=&\\text{max } \\mathbb{E}\\left[\\sum_{t'=t}^{T \\rightarrow \\infty} \\gamma^{t'-t} r_{t'} | \\pi_{\\theta}\\right], t'-t \\rightarrow n \\\\\n",
    "&=&\\text{max } \\mathbb{E}\\left[\\sum_{n=0}^{T \\rightarrow \\infty} \\gamma^n r_{t+n} | \\pi_{\\theta}\\right] \\\\\n",
    "&=& \\text{max } \\mathbb{E}[r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots + \\gamma^{T} r_{t+T} | \\pi_{\\theta}] \\in {\\rm I\\!R}.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "eFjAxWIDh36h"
   },
   "source": [
    "This can be restated in terms of the optimal Policy function $\\pi^*$ as:\n",
    "    \n",
    "$$\\pi^* = \\underset{\\pi}{\\operatorname{arg max}} J(\\pi).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected return considering a finite-horizon undiscounted return is given by:\n",
    "\n",
    "$$J(\\pi_{\\theta}) = \\mathbb{E} [\\mathcal{R}(\\tau)] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s \\in s'} \\rho(s', r | s, a).$$\n",
    "\n",
    "The expected return considering an infinite-horizon discounted return is given by:\n",
    "\n",
    "$$J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi}[\\mathcal{R}(\\tau)] = \\int_{\\tau} \\underbrace{\\mathbb{P}(\\tau | \\pi)}_{\\text{Trajectory prob.}}  \\underbrace{\\mathcal{R}(\\tau)}_{\\text{Return}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "-hmtxjzEh36i",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Glossary<a name=\"gloss\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "-uxP2mTrh36i"
   },
   "source": [
    "Main reference for this section: [Sutton & Barto c.f. [4]](#), [David Silver's RL course](https://www.davidsilver.uk/teaching/), and [Open AI](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "fyPQE97zh36i",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.1 Learning frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "CuONSlMNh36i"
   },
   "source": [
    "- **Online RL:** the algorithm uses only data from trajectories collected during runtime via direct interaction with the environment.\n",
    "\n",
    "\n",
    "- **Offline RL:** the algorithm uses an external dataset of logged transitions collected only once using some external behavioral Policy that is not assumed to be Markov. This dataset can be hand crafted by the engineer.\n",
    "\n",
    "\n",
    "- **On-Policy RL:** each update/optimization of the Policy parameters uses data collected only with the latest (most recent) Policy. Each Experience (a.k.a Transition) is used only once before it is discarded. The target Policy (the one being evaluated and improved) that the agent learns is equal to the behavior Policy (the one used for action selection). Examples of algorithms: Policy iteration, Policy evaluation, SARSA, and Policy optimization methods (DFO, Policy Gradient: PPO, A2C, etc.).\n",
    "\n",
    "\n",
    "- **Off-Policy RL:** all the data collected by the model's latest Policy is accumulated into a replay buffer for later use. The target Policy is different from the behavior Policy. This method is used to increase sample efficiency and also for: continuous exploration, learning from demonstration and parallel learning. Examples of algorithms: Q-learning, DQN, DDPG, expected SARSA. \n",
    "    - **Target Policy $\\pi$:** is the Policy being evaluated and improved during off-policy learning, i.e, is the Policy that the agent learns.\n",
    "\n",
    "    - **Behaviour Policy $b$:** is the agent's behavioral function used for action selection during the exploration phase of the off-policy learning, where the agent explores the environment to gather experiences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "iFmzvjpDh36i",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.2 Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "d2mawff_h36j"
   },
   "source": [
    "- **Markov Decision Process (MDP):** is a `discrete-time stochastic control process` that models the interaction between an agent with a stochastic environment. The process obeys the `Markov Property` and is defined as a 5-tuple:\n",
    "\n",
    "$$(S, A, R, \\mathcal{P}, \\rho_0).$$\n",
    "\n",
    "- $S \\in {\\rm I\\!R}^{d}$ is the state space.\n",
    "- $A \\in {\\rm I\\!R}^{d}$ is the action space.\n",
    "- $R: S \\times A \\times S \\rightarrow {\\rm I\\!R}$ is the reward function.\n",
    "- $\\mathcal{P}: S \\times A \\rightarrow  S \\in {\\rm I\\!R}^{d}$ is the transition probability distribution.\n",
    "- $\\rho_0 \\in S$ is the initial (starting) state distribution.\n",
    "\n",
    "\n",
    "- **Markov Property:** a state $s_t$ is a Markov state (information state) if and only if the probability of the next state $s_{t+1}$ conditioned to the current state $s_t$ is equal to the probability of the next state conditioned to the full history. Transitions do not depend on prior history, only on the current state and action. In other words: the future is independent of the past given the present. \n",
    "\n",
    "$$\\mathbb{P}[s_{t+1} | s_t] = \\mathbb{P}[s_{t+1} | s_1, \\cdots, s_T].$$ \n",
    "Also appearing as:\n",
    "$$\\mathbb{P}(r, s | s_t, a_t) = \\mathbb{P}(r, s | \\mathcal{H}_t, a_t).$$\n",
    "\n",
    "- **State space ($S$):** the set of all possible states of the environment.\n",
    "\n",
    "\n",
    "- **Action space ($A$):** the set of all possible actions the agent can select at each time step.\n",
    "    - **Discrete action space:** allows only a `limited number of actions` (Atari, Go, Chess, left or right in cart-pole env.).\n",
    "    - **Continuous action space:** allows a `continuous range of actions` (turning the wheel of a self-driving car). Here, actions are encoded as real-valued vectors.\n",
    "\n",
    "\n",
    "- **Reward function ($R$):** encodes the reward signal $r_t$: $$r_{t} = R(s_t, a_t, s_{t+1}) \\in {\\rm I\\!R}.$$\n",
    "\n",
    "\n",
    "- **Transition probability distribution ($\\mathcal{P}$):** a function that gives the conditional probability of ending up in state $s_{t+1}$ with reward signal $r_{t+1}$ after taking action $a_t$ in state $s_t$. It can be encoded as a matrix of real numbers. Every model-based RL framework has a transition probability function, while model-free RL does not.\n",
    "\n",
    "$$\\mathcal{P}(S) = \\mathbb{P}(r_{t+1}, s_{t+1} | s_t, a_t) \\in {\\rm I\\!R}^{d}.$$\n",
    "\n",
    "\n",
    "- **Initial (start)-state distribution ($\\rho_0$):** the initial state $s_0$ is sampled from a probability distribution $\\rho_0$.\n",
    "\n",
    "$$s_0 \\sim \\rho_0(\\cdot).$$ \n",
    "\n",
    "The squiggle sign $\\sim$ (tilde) means `\"sampled from\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "N0OC3Xlih36j",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.3 Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "7NfkKEEHh36k"
   },
   "source": [
    "A markov chain share the markov property with the markov decision process, however, it differs in the fact that it is a $2$-tuple \n",
    "\n",
    "$$(S, \\mathcal{P})$$ \n",
    "\n",
    "containing only the state and transition probability distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "WWHAkpcrh36k",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.4 Agent, Environment and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "tUZuJIXyh36k"
   },
   "source": [
    "- **Agent:** any mechanism or thing (robot, actuator, sensor, etc.) that can perform an action in an environment given the current state or observation.\n",
    "\n",
    "\n",
    "- **Environment:** is the agent's world. At every time step the env. returns an observation (environment state) and a reward signal.\n",
    "\n",
    "\n",
    "- **Model:** the agent's representation of the environment. The model predicts what the environment will do next. \n",
    "    - **State transition model ($\\mathcal{P}$):** predicts the next state, i.e, the environment dynamics. It gives the probability of being in the next state provided the previous state and action. The agent can learn the transition probability distribution:Â  Â \n",
    "    \n",
    "      $$\\mathcal{P}_{ss'}^a = \\mathbb{P}\\Big[S' = s' = s_{t+1} | S=s_t, A=a_t\\Big] \\in {\\rm I\\!R}^{d}.$$\n",
    "\n",
    "    - **Reward model ($\\mathcal{R}$):** predicts the next (immediate) reward signal. Gives the expected reward given the current state and action. \n",
    "    \n",
    "    $$\\mathcal{R}_s^a = \\mathbb{E}\\Big[R (\\tau) = G_t | S = s_t, A = a_t\\Big] \\in {\\rm I\\!R}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "atteyA1mh36l",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.5 Agent types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "pURMFXrnh36l"
   },
   "source": [
    "- **Value based:** stores/learn the Action-Value function (Q-value). The Policy function is implicit.\n",
    "- **Policy based:** stores/learn the Policy function directly. The user can either define a deterministic Policy, or the Policy can be learned in a deep RL framework via some neural network. There's no Action-Value function (Q-value) here!\n",
    "- **Actor-Critic:** the agent stores/learns both the Policy function and the Action-Value function (Q-value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "sEjDKD8th36l",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.6 Environment types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "yqWYLVqgh36l"
   },
   "source": [
    "- **Fully observable env.:** all states of the environment are accessible to the agent. \n",
    "\n",
    "$$O_t = S^e_t = S^a_t.$$ \n",
    "\n",
    "Meaning: the observation that the agent sees is equal to the environment state and the agent state. Formally, this is a `Markov decision process (MDP)`.\n",
    "\n",
    "- **Partially observable env.:** only some states of the env. are accessible to the agent who indirectly observes environment. Formally, this is a `partially observable Markov decision process (POMDP)`.\n",
    "    - Agent must construct its own state representation $S_t^a$.\n",
    "      - 1st approach or naive approach considers the agent state as the whole history: $S_t^a = \\mathcal{H}_t$.\n",
    "      - 2nd approach uses Beliefs of environment state (bayesian  approach), i.e, build a probability distribution for the agent state: \n",
    "\n",
    "      $$S_t^a = [\\mathbb{P}(S_t^e = s^1), \\cdots, \\mathbb{P}(S_t^e = s^n)].$$ \n",
    "\n",
    "      Where $S_t^a$ is a vector of probabilities of finding the environment state in the state $s^1$ to $s^n$.\n",
    "      - 3rd approach uses a recurrent neural network: \n",
    "\n",
    "      $$S_t^a = \\sigma(S_{t-1}^a W_s + O_t W_o).$$\n",
    "\n",
    "- **Deterministic env.:** the next `state is completely determined` by the current state and action. \n",
    "    - **Process is static:** the environment remains `stationary during runtime` and only changes upon actions. $$s_{t+1} = f(s_t, a_t) \\in {\\rm I\\!R}^{d_s} \\text{ (For a deterministic environment)}.$$\n",
    "\n",
    "- **Stochastic env.:** the next `state is randomly sampled` from a probability distribution.\n",
    "    - **Process is dynamic:** `state and reward signal changes` on its own during runtime. $$s_{t+1} \\sim \\mathbb{P}(r_{t+1}, s_{t+1} | s_t, a_t) \\in {\\rm I\\!R}^{d_s} \\text{ (For a stochastic environment)}.$$\n",
    "    \n",
    "- **Episodic:** the agent's experience is divided into periods or `time steps`.\n",
    "\n",
    "- **Single or multiple agent.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "dsbtRlGkh36m",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.7 State space and observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "pypR-FQ5h36m"
   },
   "source": [
    "- **State space ($S$):** the set of all possible states of the environment.\n",
    "\n",
    "\n",
    "- **Observation ($O_t$):** a characteristic of the environment not necessarily accessible to the agent.  \n",
    "\n",
    "    - **Environment State ($O_t = S_t^e \\in S$):** the private representation of the environment. The env. can be at only one state at a given time step.\n",
    "    \n",
    "    - **Agent State ($O_t = S_t^a = s_t \\in S$):** a function of the history $\\mathcal{H}_t$ used to train/learn the Policy.\n",
    "\n",
    "The state can be deterministic, i.e., the next state is completely determined by the current state and action:\n",
    "\n",
    "$$s_{t+1} = f(s_t, a_t) \\in {\\rm I\\!R}^{d_s} \\text{ (For a deterministic environment)}.$$\n",
    "\n",
    "The state can be stochastic, i.e., the next state is randomly sampled from a probability distribution:\n",
    "\n",
    "$$s_{t+1} \\sim \\mathbb{P}(r_{t+1}, s_{t+1} | s_t, a_t) \\in {\\rm I\\!R}^{d_s} \\text{ (For a stochastic environment)}.$$\n",
    "\n",
    "Where $\\mathbb{P}(r_{t+1}, s_{t+1} | s_t, a_t)$ is the state transition probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.8 Action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "MxY1ipo8h36n"
   },
   "source": [
    "- **Action space ($A$):** the set of all possible actions the agent can select at each time step.\n",
    "    - **Discrete action space:** allows only a `limited number of actions` (Atari, Go, Chess).\n",
    "    - **Continuous action space:** allows a `continuous range of actions` (self-driving car). Here, actions are encoded as real-valued vectors.\n",
    "    \n",
    "    \n",
    "- **Action ($a_t \\in A$):** The agent's decision making in the env. The environment goes from state $s_t$ to state $s_{t+1}$ under action $a_t$. The agent can `act randomly` or `act greedily`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For either discrete or continuous action space, actions are obtained from the Policy function according to:\n",
    "\n",
    "$$a_t = \\pi(s_t) \\equiv \\mu(s_t) \\in {\\rm I\\!R}^{d_a} \\text{ (Action for a deterministic policy)}.$$\n",
    "$$a_t \\sim \\pi(\\cdot | s_t) = \\mathbb{P}[A_t = a_t | S_t = s_t] \\in {\\rm I\\!R}^{d_a} \\text{ (Action sample for a stochastic policy)}.$$\n",
    "\n",
    "And for parameterized policies:\n",
    "\n",
    "$$a_t = \\pi_{\\theta}(s_t) \\equiv \\mu_{\\theta}(s_t) \\in {\\rm I\\!R}^{d_a} \\text{ (Action for a parameterized deterministic policy)}.$$\n",
    "$$a_t \\sim \\pi_{\\theta}(\\cdot | s_t) = \\mathbb{P}[A_t = a_t | S_t = s_t] \\in {\\rm I\\!R}^{d_a} \\text{ (Action sample for a parameterized stochastic policy)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When the Policy function is implicit, i.e, when the agent does not have direct access to the Policy, actions are computed from the $Q$-value according to:\n",
    "\n",
    "$$a(s_t)=\\underset{a}{\\operatorname{arg\\,max}} \\ Q(s_t, a_t) \\text{ (Discrete Action space)}.$$\n",
    "\n",
    "$$a(s_t) \\approx \\text{arg} \\ Q(s_t, \\mu(s_t)) \\text{ (Continuous Action space)}.$$\n",
    "\n",
    "In this case, the Policy is deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "sIDun499h36n",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.9 State transition probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "lvPz7Z6nh36r"
   },
   "source": [
    "Is a function that gives the conditional probability of ending up in state $s_{t+1}$ with reward signal $r_{t+1}$ after taking action $a_t$ in state $s_t$. It can be encoded as a matrix of real numbers. Every model-based RL framework has a transition probability function, while model-free RL does not.\n",
    "\n",
    "$$\\mathcal{P}(S) = \\mathbb{P}(r_{t+1}, s_{t+1} | s_t, a_t) \\in {\\rm I\\!R}^{d}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "Uaqm3w40h36r",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.10 History/Trajectory/Episode/Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "cVsVxw94h36s"
   },
   "source": [
    "- **History ($\\mathcal{H}_t$) a.k.a Trajectory {$\\tau_i$} Episode or Rollout:** is a sequence of time steps (**transitions or experiences**) from an initial state $s_0$ to a terminal state $s_T$ of an episodic environment. Each time step contains one observation (state) $s_t$, one action $a_t$, and one reward signal $r_t$. **Think of it as an entire gameplay**.\n",
    "\n",
    "$$\\mathcal{H}_t \\doteq \\mathcal{D}_k=\\{\\tau_i\\} = (s_0, a_0, r_1, s_1, \\cdots , s_{T-1}, a_{T-1}, r_T, s_T) \\in Tuple.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "7ppO_NXVh36s",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.11 Horizon and Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "yePev0X0h36s"
   },
   "source": [
    "- **Horizon ($h$):** the terminal state of an episode. In a finite horizon, there can be a `non-stationary optimal policy`. In an infinite horizon, there can be a `stationary optimal policy`.\n",
    "\n",
    "\n",
    "- **Experience a.k.a Transition:** a tuple consisting of the current state $s_t$, current action $a_t$, current reward signal $r_t$, next state $s_{t+1}$ and an optional done (boolean) value $d_t$.\n",
    "\n",
    "\n",
    "- **Experience replay buffer:** a memory card of collected past experiences. Used in off-policy algorithms such as Q-learning, DDPG, D4PG, MADDPG, TD3, SAC, and ACER.\n",
    "    \n",
    "    $$(s_t, a_t, r_{t}, s_{t+1}, d_{t}), (s_{t+1}, a_{t+1}, r_{t+1}, s_{t+2}, d_{t+1}), \\cdots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "A82rHjoVh36s",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.12 Reward signal, Reward function and Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "QxwpiFL1h36t"
   },
   "source": [
    "- **Reward function ($R$):**  encodes the reward signal $$r_{t} = R(s_t, a_t, s_{t+1}) \\in {\\rm I\\!R}.$$\n",
    "\n",
    "\n",
    "- **Reward signal ($r_t$):** a scalar feedback from the environment, a measure of how good the current state is after taking action $a_{t-1}$. It defines the goal of the RL problem. It drives the agent's action in the future, but do not influence how the agent decides current action. It is encoded as a `single scalar value` (positive or negative) or as a `function of state and action`.\n",
    "\n",
    "$$r_t \\in {\\rm I\\!R}.$$\n",
    "$$r_{t} = R(s_t, a_t, s_{t+1}) \\in {\\rm I\\!R}.$$\n",
    "\n",
    "\n",
    "- **Return ($G_t$):** the total (comulative) sum of rewards over one episode (or trajectory) starting from state $s_t$ onwards. The agent's goal is to maximize the expected return or cumulative reward.\n",
    "\n",
    "    - **Finite-horizon undiscounted return:** $$\\mathcal{R}(\\tau) \\doteq G_t = \\sum_{t=0}^T r_t \\in {\\rm I\\!R}.$$\n",
    "    \n",
    "    - **Infinite-horizon discounted return:** $$\\mathcal{R}(\\tau) \\doteq G_t = \\sum_{n=0}^{\\infty} \\gamma^n r_{t+n+1} \\in {\\rm I\\!R} = r_{t+1} + \\gamma G_{t+1}. \\text{ Using Recursion.}$$ Where $\\gamma$ is known as discount factor.\n",
    "\n",
    "\n",
    "- **Discount factor ($\\gamma$):** is a scalar parameter used to guarantee the convergence of the infinite series above.\n",
    "\n",
    "$$\\gamma \\in [0, 1] \\in {\\rm I\\!R}, \\text{where } 1 \\text{ means no discount}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.13 Performance objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: the following definition, using the integral sign $\\int (\\cdot)$, consider the infinite-horizon discounted return function. For the finite-horizon undiscounted return, one should replace the integral sign by the summation sign $\\sum (\\cdot)$.**\n",
    "\n",
    "Every model-based RL framework has a transition probability function, and every trajectory has an associate probability and cumulative reward (return). **If both the environment transitions and the Policy function are stochastic** (given by a probability distribution), the probability of a trajectory with $T$ time steps under Policy $\\pi$ is given by: \n",
    "\n",
    "$$\\mathbb{P}(\\tau | \\pi) = \\underbrace{\\rho_0(s_0)}_{\\text{start-state dist.}} \\prod_{t=0}^{T-1} \\underbrace{\\underbrace{\\mathbb{P}(r_{t+1}, s_{t+1}|s_t,a_t)}_{\\text{State transition prob.}}}_{\\text{Env. model.}} \\cdot \\underbrace{\\underbrace{\\pi(a_t | s_t)}_{\\text{Action prob.}}}_{\\text{Control function.}}.$$\n",
    "\n",
    "In the framework of model-based RL, where a state transition probability distribution is known, one can then write the expected return (for whichever measure) as:\n",
    "\n",
    "$$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}[\\mathcal{R}(\\tau)] = \\int_{\\tau} \\underbrace{\\mathbb{P}(\\tau | \\pi)}_{\\text{Trajectory prob.}}  \\underbrace{\\mathcal{R}(\\tau)}_{\\text{Return}}.$$ \n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\tau = (s_0, a_0, s_1, a_1, \\dots)$ represents a trajectory (a sequence of states and actions).\n",
    "- $\\pi(a_t, s_t)$ denotes the Policy function, i.e, the stochastic action probability distribution a.k.a control function.\n",
    "- $\\mathbb{P}(r_{t+1}, s_{t+1}|s_t,a_t)$ denotes the stochastic state transition probability distribution a.k.a the environment model.\n",
    "- $\\mathbb{P(\\tau | \\pi)}$: is the probability of getting a trajectory $\\tau$ with $T$ time steps acting according to policy $\\pi$.\n",
    "- $\\rho_0$ denotes the initial (start)-state probability distribution. The initial state $s_0$ is sampled from the probability distribution: $s_0 \\sim \\rho_0(\\cdot).$\n",
    "- $\\mathcal{R}(\\tau) = \\sum_{t=0}^{T} \\gamma^t r_t$ is the cumulative (discounted) reward for the trajectory over an episode.\n",
    "- $\\gamma \\in [0, 1]$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a neural network (NN) is used as the expressive nonlinear function approximator to estimate/represent a parameterized Policy function $\\pi_{\\theta}$ (the parameters $\\theta$ are the weights of the NN), the equation above can be rewritten as:\n",
    "\n",
    "$$J(\\pi_{\\theta}) =  \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau)] = \\int_{\\tau} \\underbrace{\\mathbb{P}(\\tau | \\pi_{\\theta})}_{\\text{Trajectory prob.}}  \\underbrace{\\mathcal{R}(\\tau)}_{\\text{Return}}.$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\pi_{\\theta}$ is the parameterized policy represented by a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "9JSZ-qo9h36t",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.14 Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "zj3ixkARh36u"
   },
   "source": [
    "- **Policy $\\pi$:** is a function that maps one state to an action and gives the action probabilities. The agent's goal is to find the optimal Policy that drives actions to the highest reward. The Policy can be encoded as a `simple function`, a `lookup table` (tabular methods), or as a parameterized neural network $\\pi_{\\theta}$. The mapping is denoted by: \n",
    "\n",
    "$$ \\pi: S \\rightarrow A.$$\n",
    "\n",
    "- **Target Policy $\\pi$:** is the Policy being evaluated and improved, i.e, is the Policy that the agent learns.\n",
    "\n",
    "\n",
    "- **Behaviour Policy $b$:** is the agent's behavioral function used for action selection during off-policy learning.\n",
    "\n",
    "\n",
    "- **Deterministic Policy ($\\pi(s_t)$)**: the policy always gives the same action for the same input state. The action is computed according to: \n",
    "\n",
    "$$a_t = \\pi(s_t) \\equiv \\mu(s_t) \\in {\\rm I\\!R}^{d_a}.$$\n",
    "\n",
    "\n",
    "- **Stochastic Policy ($\\pi(a_t |s_t)$):** is a conditional probability distribution over actions, from which actions are sample from according to: \n",
    "\n",
    "$$a_t \\sim \\pi(a_t |s_t) = \\mathbb{P}[A_t = a_t | S_t = s_t] \\in {\\rm I\\!R}^{d_a}.$$ \n",
    "\n",
    "The two most common types of stochastic policies are the `categorical policies` and the `diagonal Gaussian policies`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Parameterized Deterministic Policy ($\\pi_{\\theta}(s_t)$):** the policy is represented by a neural network (NN) in either discrete or continuous action space. The action is computed according to: $$a_t = \\pi_{\\theta}(s_t) \\doteq \\mu_{\\theta}(s_t) \\equiv \\pi (s_t, \\theta) \\in {\\rm I\\!R}^{d_a}.$$\n",
    "    - **Discrete action space:** in practice, it is more common to use stochastic policies for discrete action spaces and deterministic policies for continuous action spaces.\n",
    "    - **Continuous action space:** in Actor-Critic NNs, the final layer of the Actor network would have `act_dim` $=n$ `neurons` to bound the action space corresponding to a $n$-dimensional vector. For example, each component of the vector could be a torque applied to a joint (actuator) of a robotic arm with values in the range $(-1,1)$. The final layer usually has a `linear or Tanh activation function`. The final layer of the Critic network would have only one neuron (and linear activation function) corresponding to the State-Action value.\n",
    "        - `Algorithms:` DDPG, TD3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Parameterized Stochastic Policy ($\\pi_{\\theta}(\\cdot |s_t)$):** the policy is represented by a neural network (NN) in either discrete or continuous action space. The final layer of the NN is a conditional distribution. The action sample is computed according to: $$a_t \\sim \\pi_{\\theta}(\\cdot |s_t) \\doteq \\pi(a_t |s_t, \\theta) \\in {\\rm I\\!R}^{d_a}.$$\n",
    "  - **Categorical Policy (discrete action spaces):** uses a neural network (NN) that maps input states to a vector of logits for each action followed by a `softmax activation function` to convert logits into probabilities (categorical distribution). Just like in a classifier, the `number of neurons in the last layer of the NN is equal to the number of actions`.\n",
    "    - `Algorithms:` TRPO, PPO, A2C.\n",
    "\n",
    "  - **Diagonal Gaussian Policy (continuous action spaces):** uses a neural network that maps input states to a mean value $\\mu_{\\theta}(s)$ and a vector of log standard deviations, log $\\sigma_{\\theta}(s) \\in [-\\infty, \\infty]$, representing the diagonal covariance matrix $\\Sigma$ of a diagonal Gaussian distribution.\n",
    "    - `Algorithms:` TRPO, PPO, A2C, SAC.  \n",
    "    - **Example 1:** Consider, for example, the problem of a self-driving car where actions such as Steering Angle and Throttle are modeled as continuous values. A Parametrized Stochastic Policy network would contain two pair of neurons in the last layer. Each pair contains one neuron for the mean and another for the variance. \n",
    "      - `The activation function for the mean is often Tanh`, to constrain the actions such as steering or throttle within a specific range (e.g., -1 to 1).\n",
    "      - The variance is constrained to be positive, typically using an `exponential or softplus activation function`.\n",
    "      - The mean and variance (or equivalently the covariance matrix for multiple actions) define a `multivariate normal distribution`. The variance is related to the spread of the distribution, while the mean determines its central location. The network samples actions from this distribution during training.\n",
    "    - **Example 2:** the policy for a single robotic arm can have two neurons in the output layer of the NN, one neuron for the mean (representing the desired action) and another neuron for the standard deviation (representing the degree of stochasticity or uncertainty in the action).\n",
    "    - The action sample is computed according to: $$a = \\mu_{\\theta}(s) + \\sigma_{\\theta}(s) \\odot z .$$\n",
    "        - $z \\sim \\mathcal{N}(0, I)$ denotes a vector of noise from a spherical Gaussian distribution.\n",
    "        - $\\odot$ denotes the element-wise product between two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "yB9V4Ks_h36v",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.15 Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "oMD2lV15h36v"
   },
   "source": [
    "The value function gives the expected cumulative future reward a.k.a expected return an agent can get. The expectation is used to yield a normalized estimate of the reward. There are four types of value functions, as outlined below.\n",
    "\n",
    "**Note:** value functions with `time-dependence` denotes the use of `finite-horizon undiscounted return`, while value functions `without time-dependence` denotes the use of `infinite-horizon discounted return`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ynnIep8Mh36v"
   },
   "source": [
    "- **Action-Value Function a.k.a state-action value function $Q^{\\pi}(s_t, a_t)$:** gives the expected cumulative future reward a.k.a expected return an agent will get if starting from state $s_t=s_0$, taking arbitrary action $a_t=a_0$ (not necessarily from the Policy), and then forever acting under Policy $\\pi$. In other words, it evaluates how good is action $a_t \\in A$ in state $s_t \\in S$.\n",
    "\n",
    "$$Q^{\\pi}(s_t, a_t): S \\rightarrow {\\rm I\\!R}.$$\n",
    "$$Q^{\\pi}(s_t, a_t) = \\mathbb{E}_{\\tau \\sim \\pi}[\\mathcal{R}(\\tau) | s_0 = s_t, a_0 = a_t] = \\frac{1}{N}\\sum_{n=1}^N \\mathcal{R}_t^n.$$\n",
    "\n",
    "When the Policy function is implicit, i.e, when the agent does not have direct access to the Policy, actions are computed from the $Q$-value according to:\n",
    "\n",
    "$$a(s_t)=\\underset{a}{\\operatorname{arg\\,max}} \\ Q(s_t, a_t) \\text{ (Discrete Action space)}.$$\n",
    "\n",
    "$$a(s_t) \\approx \\text{arg} \\ Q(s_t, \\mu(s_t)) \\text{ (Continuous Action space)}.$$\n",
    "\n",
    "In this case, the Policy is deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **State-Value Function $V^{\\pi}(s_t)$:** gives the expected cumulative future reward a.k.a expected return from state $s_t = s$ onwards while acting according to Policy $\\pi$.\n",
    "\n",
    "$$V^{\\pi}(s_t): S \\rightarrow {\\rm I\\!R}.$$\n",
    "$$V^{\\pi}(s_t) = \\mathbb{E}_{\\tau \\sim \\pi}[\\mathcal{R}(\\tau) | s_t = s].$$\n",
    "$$V^{\\pi}(s_t) = \\mathbb{E}_{a_t \\sim \\pi(s_t)}[Q^{\\pi}(s_t, a_t)].$$\n",
    "\n",
    "\n",
    "- **Optimal Action-Value Function $Q^*(s_t, a_t)$:** gives the expected cumulative future reward a.k.a expected return an agent will get if starting from state $s_t=s$, taking arbitrary action $a_t = a$, and then forever acting under the optimal Policy.\n",
    "\n",
    "    $$Q^*(s_t, a_t) = \\underset{\\pi}{\\operatorname{max}} \\ \\mathbb{E}_{\\tau \\sim \\pi} [\\mathcal{R}(\\tau) |s_t = s, a_t = a].$$\n",
    "    $$a^*(s_t) = \\underset{a}{\\operatorname{arg\\,max}} \\ Q^*(s_t, a_t).$$\n",
    "    \n",
    "    \n",
    "- **Optimal State-Value Function $V^*(s_t)$:** gives the expected cumulative future reward a.k.a expected return from state $s_t = s$ onwards while acting according to the optimal Policy.\n",
    "\n",
    "$$V^*(s_t)=\\underset{\\pi}{\\operatorname{max}} \\ \\mathbb{E}_{\\tau \\sim \\pi}[\\mathcal{R}(\\tau)|s_t = s].$$\n",
    "$$V^*(s_t)=\\underset{a}{\\operatorname{max}} \\ Q^*(s_t, a_t).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "1C-JntfKh36w",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.16 Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "J01iJ58rh36w"
   },
   "source": [
    "Bellman equations correspond to a set of four self-consistency equations.\n",
    "\n",
    "- **The state-value function can be decomposed into:**\n",
    "\n",
    "$$V^\\pi(s) = \\underset{\\underset{a \\sim \\pi}{s' \\sim \\mathcal{P}}}{\\mathbb{E}} \\ [r(s, a) + \\gamma V^{\\pi}(s') | S = s].$$\n",
    "\n",
    "- **The Q-value function can be decomposed into:**\n",
    "\n",
    "$$Q^{\\pi}(s, a) = \\underset{s' \\sim \\mathcal{P}}{\\operatorname{\\mathbb{E}}} \\ \\Bigg[r(s,a) + \\gamma \\mathbb{E}_{a'\\sim\\pi}\\Big[Q^{\\pi}(s', a')\\Big] | S = s, A=a\\Bigg].$$\n",
    "\n",
    "\n",
    "- **Bellman equation for the optimal state-value function:**\n",
    "\n",
    "$$V^*(s) = \\underset{a}{\\operatorname{max}} \\ \\underset{s' \\sim \\mathcal{P}}{\\operatorname{\\mathbb{E}}} \\ [r(s, a) + \\gamma V^*(s') | S = s].$$\n",
    "\n",
    "\n",
    "- **Bellman equation for the optimal Q-value function:**\n",
    "\n",
    "$$Q^*(s, a) =\\underset{s' \\sim \\mathcal{P}}{\\operatorname{\\mathbb{E}}} \\ \\Bigg[r(s,a) + \\gamma \\underset{a'}{\\operatorname{max}} \\ \\Big[Q^*(s', a')\\Big] | S = s, A=a\\Bigg].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "_wanrcuDh36w"
   },
   "source": [
    "- **Bellman backup:** is the term in the right-hand side (RHS) of the Bellman equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "N3b8Dwd6h36x"
   },
   "source": [
    "**Proof:**\n",
    "\n",
    "\\begin{eqnarray}\n",
    "V(s) &=& \\mathbb{E} [\\mathcal{R}_t| S_t = s] \\\\\n",
    "&=& \\mathbb{E} [r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + ... | S_t = s] \\\\\n",
    "&=& \\mathbb{E} [r_t + \\gamma  (r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ...) | S_t = s] \\\\\n",
    "&=& \\mathbb{E} [r_t + \\gamma  \\mathcal{R}_{t+1} | S_t = s] \\\\\n",
    "&=& \\mathbb{E} [r_t| S_t = s] + \\mathbb{E}[\\gamma \\mathcal{R}_{t+1} | S_t = s] \\  \\color{green}{\\text{(using property 1).}} \\\\\n",
    "&=& \\mathbb{E} [r_t| S_t = s] + \\gamma \\mathbb{E}[\\mathcal{R}_{t+1} | S_t = s] \\ \\color{green}{\\text{(using property 2).}}\\\\\n",
    "&=& \\mathbb{E} [r_t| S_t = s] + \\gamma V(S_{t+1}) \\ \\color{green}{\\text{(using the definition of state-value function).}}\\\\\n",
    "&=& \\mathbb{E} [r_t | S_t = s] + \\mathbb{E}[\\gamma V(S_{t+1})| S_t = s] \\ \\color{green}{\\text{(using property 3).}}\\\\\n",
    "&=& \\mathbb{E} [r_t + \\gamma V(S_{t+1}) | S_t = s] \\ \\color{green}{\\text{(using property 1).}} \n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "BjBX2tjKh36x"
   },
   "source": [
    "Where it was used $\\mathbb{E}[G_{t+1}|S_t=s] = V(S_{t+1})$ and the following properties:\n",
    "\n",
    "1. The expected value of a sum is the sum of the expectations: \n",
    "\n",
    "$$\\mathbb{E}[X+Y] = \\mathbb{E}[X]+ \\mathbb{E}[Y].$$ \n",
    "\n",
    "\n",
    "2. The expected value with a constant is the constant multiplied by the expected value: \n",
    "\n",
    "$$\\mathbb{E}(\\gamma X) = \\gamma \\mathbb{E}(X).$$\n",
    "\n",
    "\n",
    "3. [Law of iterated expectations a.k.a law of total expectation](https://en.wikipedia.org/wiki/Law_of_total_expectation): the expected value of an expected value is \n",
    "\n",
    "$$\\mathbb{E}[\\mathbb{E}[X]]=\\mathbb{E}[X],$$ \n",
    "\n",
    "with consequences such as $$\\mathbb{E}(2X\\mathbb{E}(X)) = 2E(X\\mathbb{E}(X))=2\\mathbb{E}(X)\\mathbb{E}(X),$$\n",
    "\n",
    "which can be used to prove the identity\n",
    "\n",
    "$$\\text{Var}(X) = \\mathbb{E}((X-\\mathbb{E}(X))^2) = \\mathbb{E}(X^2) - (\\mathbb{E}(X))^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "huMlrhJ0h36y"
   },
   "source": [
    "Using Bellman equation, one can write:\n",
    "\n",
    "$$V(s)= r +\\gamma \\sum_{s' \\in S} \\mathcal{P}{ss'}V(S'),$$\n",
    "\n",
    "and also\n",
    "\n",
    "$$V= r+\\gamma \\mathcal{P}V.$$\n",
    "\n",
    "This is a linear equation whose solution is also the solution for the MDP:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "V &=& r+\\gamma \\mathcal{P}V \\\\ \n",
    "&\\implies& r = V-\\gamma \\mathcal{P}V \\\\\n",
    "&\\implies& r = (\\mathcal{I}-\\gamma \\mathcal{P})V \\\\\n",
    "&\\implies& (\\mathcal{I}-\\gamma \\mathcal{P})^{-1} r = (\\mathcal{I}-\\gamma \\mathcal{P})^{-1}(\\mathcal{I}-\\gamma \\mathcal{P})V = V\n",
    "\\end{eqnarray}\n",
    "\n",
    "The computational complexity for calculating the inverse matrix is $\\mathcal{O}(n^3)$, meaning that direct solution is only possible for small MRP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "5r4-ZdKJh36y"
   },
   "source": [
    "The Bellman Optimality Equation is non-linear and, therefore, cannot be solved by linear algebra methods such the previous matrix inversion. And, in general, it has no [closed form solution](https://en.wikipedia.org/wiki/Closed-form_expression). To solve this recursive equation, one needs iterative methods (dynamic programming) such as:\n",
    "- Value iteration (on-Policy learning).\n",
    "- Policy iteration (on-Policy learning).\n",
    "- Q-learning (almost always off-policy learning).\n",
    "- Sarsa (on-Policy learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "aiD8CTOxh36y",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.17 Advantage Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "kQ7OKe9th36y"
   },
   "source": [
    "- **Advantage Function:** \n",
    "\n",
    "$$A^{\\pi}(s_t, a_t) = Q^{\\pi}(s_t, a_t) - V^{\\pi}(s_t).$$\n",
    "\n",
    "The Advantage function is the difference between the expected return (Action-Value function) and the baseline estimate. It computes an estimate of the \"relative value of the selected action\". The baseline is the State Value function that gives a noisy estimate of the return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.18 Exploitation vs Exploration: the RL tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Exploitation:** chooses the action that gives the maximum reward for the current time step, i.e, uses known information (collected experiences) to maximize reward choosing the best actions.\n",
    "- **Exploration:** sacrifices reward to find more information about the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.19 $\\epsilon$-greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The $\\epsilon$-greedy policy is used in discrete action spaces.\n",
    "\n",
    "**Algorithm (adapted from Hado van Hasselt, UCL):**\n",
    "\n",
    "- Generate a random number $\\epsilon \\in [0,1]$.\n",
    "\n",
    "- Sample a greedy action (exploitation) with probability $1-\\epsilon$:\n",
    "\n",
    "$$a(s_t)=\\underset{a \\in A}{\\operatorname{arg\\,max}} \\ Q_{\\tau}(s_t, a_t) \\text{ (Discrete Action space)}.$$\n",
    "\n",
    "$$a(s_t) \\approx \\text{arg} \\ Q(s_t, \\mu(s_t)) \\text{ (Continuous Action space)}.$$\n",
    "\n",
    "- Else, sample a random action (exploration) with probability $\\epsilon$.\n",
    "\n",
    "**Note:** to improve exploration, the parameter $\\epsilon$ can be decreased over time (epsilon decay). A constant $\\epsilon$ may lead to suboptimal action (linear expected total regret).\n",
    "\n",
    "\n",
    "Summary:\n",
    "\n",
    "- Greedy policy is deterministic.\n",
    "\n",
    "- $\\epsilon$-greedy policy is stochastic.\n",
    " \n",
    "\\begin{equation}\n",
    "\\pi_t(a_t) = \\begin{cases}\n",
    "  (1-\\epsilon)+\\epsilon/|A|, & \\text{if }  Q_{\\tau}(s_t, a_t) = \\underset{b \\in B}{\\operatorname{max}}Q_{\\tau}(b_t).\\\\\n",
    "  \\epsilon/|A|, & \\text{otherwise. }\\\\\n",
    " \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Even though DQN uses $\\epsilon$-greedy policy (stochastic) during exploration, the policy produced by DQN after training (used in production/deployment) is deterministic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "94i0GqZlh36z",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.20 Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "VoJHVzFah36z"
   },
   "source": [
    "A concept used in off-Policy learning to compute the expectation value of the return $\\mathcal{R}$ under the target Policy $\\pi(a|s)$ using reward signals sampled from the behavior Policy $b(a|s)$:\n",
    "\n",
    "$$\\mathbb{E}_{\\pi} [\\mathcal{R}(\\tau)] = \\sum_{r\\in\\mathcal{R}(\\tau)} r \\pi(a|s) = \\sum_{r\\in\\mathcal{R}(\\tau)} r \\pi(a|s) \\frac{b(a|s)}{b(a|s)} =\\sum_{r\\in\\mathcal{R}(\\tau)} r \\rho b(a|s)= \\mathbb{E}_b [\\mathcal{R}(\\tau) \\rho] \\approx \\frac{1}{n}\\sum_{i=0}^{n-1}r_i \\rho_i.$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$ \\rho = \\frac{\\rho_0(s_0) \\prod_{t=0}^{T-1}\\mathbb{P}(r_{t+1}, s_{t+1}|s_t,a_t) \\cdot \\pi(a_t | s_t)}{\\rho_0(s_0) \\prod_{t=0}^{T-1}\\mathbb{P}(r_{t+1}, s_{t+1}|s_t,a_t) \\cdot b(a_t | s_t)} = \\prod_{t=0}^{T-1} \\frac{\\pi(a_t | s_t)}{b(a_t | s_t)},$$\n",
    "\n",
    "\n",
    "and $n$ denotes the number of time steps in a trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "sd8-_RiMh36z",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.21 Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "K7XNmFCBh360"
   },
   "source": [
    "**In statistics**: bootstrapping refers to a statistical technique that consists of `sampling with replacement` while pretending that the sample dataset is the population dataset.\n",
    "\n",
    "- Problem: suppose we want to estimate the average height of people in a large population. It is not feasible to measure the height of each person individually.\n",
    "    - First solution: use Monte Carlo by sampling 1000 people randomly and then taking the average of the sample dataset. The mean average will be closer to the population average given the law of large numbers.\n",
    "    - Second solution: use Bootstrapping. Pretend that the sample dataset is the population dataset, then apply the Monte Carlo method, i.e., sample 1000 people randomly, and then take the average of the sample dataset. Now, repeat (say 100 times) the same procedure with replacement. In the end, there will be 100 estimates of the height. Finally, take the standard deviation from those estimates.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In reinforcement learning:** the agent updates its current value by using estimates/predictions of future values.\n",
    "\n",
    "Temporal Difference (TD) learning is a bootstrapping method. Example: \n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha(R_{t+1}+\\gamma Q(s',a')-Q(s,a)),$$\n",
    "\n",
    "where $R_{t+1}+\\gamma Q(s',a')$ is known as the TD target Q-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "G56Z-US2h360",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.22 Deadly triad issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "F5DEgcVCh360"
   },
   "source": [
    "The deadly triad is a drawback caused by instabilities and/or divergence in the value function when using bootstrapping, off-policy learning, and function approximations such as neural networks in the same algorithm.\n",
    "\n",
    "Circumventions: target networks, deep double q-learning, and prioritized replay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "-pA2WLBIJjPf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. Taxonomy (extended)<a name=\"tax\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "sh-W2eBOh361"
   },
   "source": [
    "Main reference for this section: [John Schulman's PhD Thesis](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-217.html), [David Silver's RL course](https://www.davidsilver.uk/teaching/), and [Sergey Levine's DRL course](http://rail.eecs.berkeley.edu/deeprlcourse/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This section is dedicated to understand the below taxonomy from [Open AI](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Taxonomy](../assets/taxonomy.svg)\n",
    "Â© Image Credits: [Open AI](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Nw4NoP8oh361"
   },
   "source": [
    "- **Model-based RL:** there is a model of the environment for planning (lookahead), i.e, a function to predict future situations (state transitions and rewards) before they actually happen. Algorithms of this kind are more sample efficient. The agent can learn either the `Policy` and/or the `Value function`.\n",
    "\n",
    "\n",
    "- **Model-free RL:** methods are based explicitly in trial-and-error via interaction with the unknown environment. There is no model of the environment. Algorithms of this kind are less sample efficient, but easier to implement and tune. The agent can learn either the `Policy` and/or the `Value function`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "d6jNszs5h362"
   },
   "source": [
    "- **Dynamic Programming (DP) Methods:** DP is an optimization method developed by Richard Bellman (1950) that consists in splitting a complex problem into its subproblemms with optimal substructure. In RL, it exploits the structure of the problem indirectly to learn a `Value Function` using self-consistency properties of the same. Algorithms of this class are more sample-efficient and compatible with exploration and off-policy learning.\n",
    "    - **Policy Iteration:** is a `model-based` (uses a transition probability) `on-policy` (no replay buffer) and non-parametric `tabular method` in RL (data is stored in a `look-up table`) that solves (exactly) the true `Optimal Policy function` $\\pi^*$ (`Policy is deterministic`) for a Markov Decision Process (MDP). The algorithm has two interacting steps in a feedback loop until Policy converges: Policy evaluation followed by Policy improvement. The algorithm starts with a random Policy and random value function until convergence via a series of updates.\n",
    "        - `Policy evaluation:` updates the State-Value function $V^{\\pi}(s)$ given the current Policy $\\pi$. One advantage over Value iteration (nonlinear due to max operator) is that this step can be computed with a linear program without the use of dynamic programming.\n",
    "        - `Policy improvement:` updates the current Policy $\\pi$ given the updated State-Value function $V^{\\pi}(s)$. \n",
    "    - **Value Iteration:** is a `model-based` (uses a transition probability) `on-policy` (no replay buffer) and non-parametric `tabular method` in RL (data is stored in a `look-up table`) that iteratively computes (exactly) the true `optimal State-Value function` $V^*(s)$ for an MDP and extracts the optimal Policy (`Policy is deterministic`) only once using the Bellman optimality equation. Consists of two main steps: `finding optimal State-Value function` and `one Policy extraction`. The algorithm can also be modified to solve for the Action-Value function. The algorithm starts with a random value function and through an iterative process converges to the true optimal value function within a margin (threshold) of error. Finally, the optimal Policy is extracted using Bellman optimality equation. The algorithm requires dynamic programming since the optimality Bellman operator is nonlinear due to the max operator.\n",
    "        - **Q-learning:** approximate version of value iteration. Is a `model-free` (no state transition probability distribution) `off-policy` (uses a replay buffer), `temporal difference` (TD uses bootstrapping) and `tabular method` in RL (data is stored in a look-up table) that computes the `optimal Q-function` $Q^*(s_t, a_t)$ for each state-action pair. The Q-function is represented by a look-up table of Q-values for each state-action pair. It learns an implicit greedy Policy (deterministic) using a behaviour Policy (usually $\\epsilon$-greedy, i.e, stochastic). The objective function is based on the optimal Bellman equation.\n",
    "            - Examples of algorithms: C51.\n",
    "        - **SARSA:** `on-policy` value-based Temporal Difference (TD uses bootstrapping) control algorithm that combines Monte Carlo and dynamic programming methods.\n",
    "    - **Modified Policy Iteration:** the Policy evaluation step is repeated for $k$ times update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "JgyKJxuPh362"
   },
   "source": [
    "- **Policy Optimization Methods:** optimize the Policy parameters `either directly`, computing the `gradient ascent` of the performance objective $J(\\pi_{\\theta})$ (a.k.a expected return under parameterized policy), `or indirectly`, by maximizing `local approximations` of $J(\\pi_{\\theta})$. Algorithms of this class are compatible with nonlinear function approximators, such as multilayer perceptron neural networks (MLP) and recurrent neural networks (LSTM). MLP-based implementations, since they lack convolutional neural networks, are more suitable for fully-observed, non-image-based RL environments. Algorithms of this class are also more stable (less variance) than Q-learning, however, they tend to be less sample efficient, as they are `almost always based in on-policy` learning. Examples of algorithms: DFO, hill climbing, finite difference methods, likelihood ratio policy gradients.\n",
    "    - **Derivative-free Optimization (DFO) a.k.a Evolutionary algorithms:** treats the entire process of reward generation as a black box and optimize over the policy parameter vector $\\theta_i$ (that follows some distribution such as a Gaussian) to maximize the return. The inputs ($\\theta_i$) to the black box can be chosen, the outputs (rewards) can only be observed. Algorithms of this class do not scale well with the number of parameters.\n",
    "        - Examples of algorithms: \n",
    "            - Cross Entropy Method (CEM): learns a parameterized Policy with a neural network without backpropagation.\n",
    "    - **Policy Gradient:** measures the `gradient of the policy performance objective` $J(\\pi_{\\theta})$ with respect to the parameters $\\theta$ of a parameterized Policy $\\pi_{\\theta}$ that is approximated/represented by a `neural network` as the expressive nonlinear function approximator. The Policy could be either `deterministic` or `stochastic`. Algorithms of this class scale well with the number of parameters.\n",
    "        - Examples of algorithms: \n",
    "            - Finite Difference Methods.            \n",
    "            - VPG: learns On-Policy value function and On-Policy stochastic Policy.\n",
    "            - And the following actor-critic methods: A2C, A3C, DPG, DDPG, D4PG, MADDPG, TRPO, PPG, TD3, and SVPG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "o1A7sZDqh362"
   },
   "source": [
    "- **Deep RL:** uses a `deep neural network as an expressive nonlinear function approximator` to learn either a `parameterized value function`, a `parameterized Policy`, `or the model`. The optimization of the value function / policy / model can be done `end-to-end with stochastic gradient descent`.\n",
    "    - **Deep Q-Network (DQN):**  developed by DeepMind in 2015, is the enhanced version of Q-learning. Is a model-free off-policy (uses a replay buffer) approach for large-scale RL systems. It learns a parameterized $Q_{\\theta}(s_t, a_t)$ function represented by a `single neural network` (function approximator) as opposed to a look-up table to estimate the `parameterized optimal Action-Value function` $Q^*_{\\theta}(s_t, a_t)$ for a state-action pair. The Q-values of the parameterized Q-function are estimated by the neural network. To ensure stability, it uses `experience replay` (transitions are added to a replay buffer at each time step) and a snapshot of previous transitions known as the `frozen target network`. It also uses $\\epsilon$-`greedy approach` to ensure `exploitation` and for action selection. The NN's forward pass is then computed with a `mini-batch` of transitions sampled from the replay buffer to find the gradient of the loss function. The loss function is encoded by the `Bellman residual`. The original DQN is for `discrete action space`. The number of neurons in the input layer equals the observation space, while the number of neurons in the output layer equals the number of actions. The `output layer uses a linear activation function`. After training, the action given the Policy (`Policy is deterministic`) is obtained according to: \n",
    "    $$a^*(s_t) \\in {\\rm I\\!R}^{d_a} = \\underset{a}{\\operatorname{arg\\,max}} \\ Q_{\\theta}^*(s_t, a_t).$$ `Remember: time-dependence indicates finite-horizon undiscounted return`. DQN is preferable over tabular methods such as Q-learning and Value/Policy iteration which are time-consuming given the large state space required to iterate over and unfeasible (memory bottlenecks) to store values in a look-up table as the state-action space grows. Learning look-up tables is replaced by learning weights in the neural network. [\"Training a deep RL agent on 50+ Atari 2600 games in ALE for 200M frames (the standard protocol) requires 1,000+ GPU days.\"](https://ai.googleblog.com/2022/11/beyond-tabula-rasa-reincarnating.html?m=1)\n",
    "    - **Actor-Critic Methods:** a combination of `Policy Gradient` methods and `Policy/Value Iteration` methods that uses `two neural networks`, the Actor and the Critic, as expressive nonlinear function approximators.\n",
    "        - Examples of algorithms:\n",
    "            - A2C: is an On-Policy temporal difference (TD) learning method that uses a stochastic Policy (Actor network). Continuous or discrete action space.\n",
    "            - A3C: uses On-Policy learning. Continuous or discrete action space.\n",
    "            - DPG. there is On-Policy and Off-Policy versions. Continuous action space.\n",
    "            - DDPG: is a model-free (no transition probability) off-policy `actor-critic` algorithm that combines elements of policy gradient methods with deep Q-learning. DDPG is an extension of DQN for continuous action space. It uses `temporal difference learning` (bootstrapping) and `experience replay buffer` (off-policy) to learn the Q-value (represented by the Critic network). Unlike DQN, DDPG does not use $\\epsilon$-greedy policy (exploitation) for action selection. Rather, In DDPG, the behavior policy for action selection is derived from the actions generated by the Actor network (which is a deterministic target policy) with the addition of noise to encourage `exploration` in the environment.\n",
    "            - D4PG: uses Off-Policy learning. Continuous action space.\n",
    "            - MADDPG: uses Off-Policy learning. Continuous action space.\n",
    "            - TRPO: uses On-Policy learning and stochastic Policy (Actor network). Continuous or discrete action space.\n",
    "            - PPG: learns an Off-Policy value function (Critic network) and an On-Policy Policy function (Actor network).\n",
    "            - TD3: uses Off-Policy learning and deterministic Policy (Actor network). Continuous action space. Addresses DDPG's lack of stability.\n",
    "            - PPO: uses On-Policy learning and stochastic Policy. Continuous or discrete action space. Optimization indirectly maximize the performance object, instead uses a surrogate objective.\n",
    "            - SAC: uses Off-Policy learning and stochastic Policy. Continuous action space. Maximizes expected reward and entropy. Addresses DDPG's lack of stability.\n",
    "            - ACER: uses Off-Policy learning. Continuous or discrete action space.\n",
    "            - ACTKR: uses On-Policy learning. Continuous or discrete action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "baSeSMpzWcHa",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 6. Open Challenges in RL<a name=\"open\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "wwSTTRLDWcHa"
   },
   "source": [
    "- **Sparse rewards:** is when the agent only gets a reward after the end of the episode, since it does not know which of the actions in the trajectory were the cause of negative scores. Arises from the fact that we don't know the target labels that the network should produce from the input data, so the agent should learn from sparse feedback.\n",
    "    - **Reward shaping:** one solution to the `sparse rewards` problem is to craft a new reward function at each time step. However, this approach is not scalable as each new environment will need a tailored reward.\n",
    "\n",
    "- **Alignment problem:** the `reward shaping` suffers from the `alignment problem`, i.e, the policy will overfit to the specific reward function without generalizing or behaving as expected.\n",
    "    \n",
    "    \n",
    "    \n",
    "- **Reinforcement Learning with sparse rewards:**\n",
    "  - [Auxiliary Reward signals](https://arxiv.org/abs/1611.05397): augment the sparse reward signal coming from the environment by additional feedback signal.\n",
    "  - [epsilon greedy exploration](#): choose a random action with probability $p$ or pick the top action from the policy network with probability $1-p$. Does not work when the environment is too complex to explore.\n",
    "  - [Curiosity-driven exploration by self-supervised prediction](https://arxiv.org/abs/1705.05363): incentivize exploration in the environment with a forward model, i.e, encode the input state (observation) into a latent representation and then use the foward model to predict this same representation for the next frame of the env. It tries to learn the dynamics of the env. This should be used as an additional feedback signal on top of the sparse reward.\n",
    "  - [Hindsight Experience Replay (HER)](https://arxiv.org/abs/1707.01495): the idea is to make the agent learn even from unsuccessful episodes by creating a dense reward setting with modified virtual goals that will turn a failure into a sucess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyNmEIRnh363"
   },
   "source": [
    "# 7. Policy Optimization Algorithms<a name=\"algo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3Y5H3a-h364"
   },
   "source": [
    "Main reference for this section: [Sutton & Barto 2018 Chap. 13 c.f. [4]](#), [Open AI](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html), [John Schulman's PhD Thesis](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-217.html), and [Sergey Levine's DRL course](http://rail.eecs.berkeley.edu/deeprlcourse/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "gisL9_D8h369",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7.1 Derivative-free Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "EtohG7nhh369"
   },
   "source": [
    "Gradient-based reinforcement learning might be overkill to some tasks, therefore, it is worth to first investigate derivative-free optimization algorithms such as the Cross Entropy Method (CEM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "p_lBrVFZh37t",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 7.1.1 Cross Entropy Method (CEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "iV4gjvH7h37u"
   },
   "source": [
    "The Cross Entropy Method (CEM) is one example of evolutionary algorithm that works with Gaussian distributions through successful updates of the mean and variance. CEM shows better performance in the game Tetris (used as task benchmark) than Policy Gradient Methods. This may be due to the discount factor that limits the scalability of the action space since further rewards are less valuable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ECA7XntJh37u"
   },
   "source": [
    "---\n",
    "**Algorithm (Pseudocode): Cross Entropy Method Algorithm (adapted from John Schulman)**\n",
    "\n",
    "---\n",
    "\n",
    "- Input: initialize two $d$-dimensional vectors: generate a Gaussian distribution with mean $\\mu \\in \\mathbb{R}^d$, and standard deviation $\\sigma \\in \\mathbb{R}^d$.\n",
    "\n",
    "- for iteration = 1, 2, ... , n do:\n",
    "\n",
    "    - Collect $n$ samples $\\theta_i$ (the weights a.k.a parameters of the Policy) from the Gaussian distribution, i.e, $\\theta_i âˆ¼ N(Î¼, diag(Ïƒ))$.\n",
    "    \n",
    "    - Run one episode with each $\\theta_i$ and collect the return $R_i \\sim \\theta_i$.\n",
    "    \n",
    "    - Select the top $p$% of samples (e.g. $p = 20$) and store then into an \"elite set\".\n",
    "    \n",
    "    - Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining the new mean $\\mu$, and standard deviation $\\sigma$.\n",
    "    \n",
    "- end for.\n",
    "\n",
    "- Return final mean $\\mu$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mieCj6uUh37u",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7.2 [Policy Gradient Algorithms](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic9ZShibh37u"
   },
   "source": [
    "The goal of reinforcement learning is to maximize the expected cumulative reward (a.k.a expected return) under policy $\\pi_{\\theta}$:\n",
    "\n",
    "$$\\text{max } J(\\pi_{\\theta}) = \\text{max } \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau) | \\pi_{\\theta_t}].$$\n",
    "\n",
    "In Policy Gradient Algorithms, this can be achieved by directly updating/optimizing the parameters $\\theta_t$ of the parameterized Policy $\\pi_{\\theta}$ computing the `gradient ascent` of the performance objective $J(\\pi_{\\theta})$ with respect to the Policy parameters $\\theta_t$:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\theta_{t+1} &=& \\theta_t + \\alpha \\nabla_{\\theta} J(\\pi_{\\theta})|_{\\theta_t}\\\\\n",
    "&=& \\theta_t + \\alpha \\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau) | \\pi_{\\theta_t}].\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "-  $\\pi_{\\theta}$ is the parameterized Policy function represented by a neural network as an expressive nonlinear function approximation.\n",
    "\n",
    "- $\\alpha$ is the learning rate.\n",
    "  \n",
    "- $\\nabla_{\\theta} J(\\pi_{\\theta})$ denotes the gradient of `policy performance` a.k.a `policy gradient`.\n",
    "\n",
    "- $\\mathcal{R}(\\tau) \\doteq \\sum_{t=0}^T r_t \\in {\\rm I\\!R} \\text{ (Finite-horizon undiscounted return)}$ is the sum of rewards over a fixed window of time steps.\n",
    "\n",
    "- $\\mathcal{R}(\\tau) \\doteq \\sum_{t=0}^{\\infty} \\gamma^t r_{t} \\in {\\rm I\\!R} \\text{ (Infinite-horizon discounted return)}$ is the sum of all rewards ever obtained.\n",
    "\n",
    "- $\\gamma \\in [0,1]$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the infinite-horizon discounted return function, the performance objective $J(\\pi_{\\theta})$ is defined as the expected cumulative reward under a parameterized policy $\\pi_{\\theta}$, which is mathematically represented as:\n",
    "\n",
    "$$J(\\pi_{\\theta}) =  \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau)] = \\int_{\\tau} \\underbrace{\\mathbb{P}(\\tau | \\pi_{\\theta})}_{\\text{Trajectory prob.}}  \\underbrace{\\mathcal{R}(\\tau)}_{\\text{Return}},$$\n",
    "\n",
    "with the the probability of trajectory $\\tau$ under the policy $\\pi_{\\theta}$ given by\n",
    "\n",
    "$$\\mathbb{P}(\\tau | \\pi_{\\theta}) = \\underbrace{\\rho_0(s_0)}_{\\text{start-state dist.}} \\prod_{t=0}^{T-1} \\underbrace{\\underbrace{\\mathbb{P}(s_{t+1}|s_t,a_t)}_{\\text{State transition prob.}}}_{\\text{Env. model.}} \\cdot \\underbrace{\\underbrace{\\pi(a_t | s_t)}_{\\text{Action prob.}}}_{\\text{Control function.}}.$$\n",
    "\n",
    "\n",
    "Legend:\n",
    "\n",
    "- $\\tau = (s_0, a_0, s_1, a_1, \\dots)$ represents a trajectory (a sequence of states and actions).\n",
    "\n",
    "-  $\\pi_{\\theta}$ is the parameterized Policy function represented by a neural network as an expressive nonlinear function approximation.\n",
    "\n",
    "- $\\mathbb{P(\\tau | \\pi_{\\theta})}$: is the probability of getting a trajectory $\\tau$ with $T$ time steps acting according to policy $\\pi_{\\theta}$.\n",
    "\n",
    "- $\\rho_0$ denotes the initial (start)-state probability distribution. The initial state $s_0$ is sampled from the probability distribution: $s_0 \\sim \\rho_0(\\cdot).$\n",
    "\n",
    "- $\\mathcal{R}(\\tau) = \\sum_{t=0}^{T} \\gamma^t r_t$ is the cumulative (discounted) reward for the trajectory over an episode.\n",
    "\n",
    "- $\\gamma \\in [0, 1]$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6pAzicIh37v"
   },
   "source": [
    "To compute the policy gradient numerically, one must first derive an analytical expression for the policy gradient in terms of the expected value and then sample trajectories through agent-environment interaction steps. The policy gradient reads:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\nabla_{\\theta} J (\\pi_{\\theta}) &=& \\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\mathcal{R}(\\tau)] \\quad (Eq. 1.1) \\\\\n",
    "&=& \\nabla_{\\theta} \\int_{\\tau} \\mathbb{P}(\\tau | \\pi_{\\theta}) [\\mathcal{R}(\\tau)] \\quad (Eq. 1.2) \\\\\n",
    "&=& \\int_{\\tau} \\nabla_{\\theta} \\mathbb{P}(\\tau | \\pi_{\\theta})  [\\mathcal{R}(\\tau)] \\quad (Eq. 1.3) \\\\\n",
    "&=& \\int_{\\tau}  \\mathbb{P}(\\tau | \\pi_{\\theta}) \\Bigg[ \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\pi_{\\theta}))  \\mathcal{R}(\\tau) \\Bigg] \\quad (Eq. 1.4) \\\\\n",
    "&=& \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\Bigg[ \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\pi_{\\theta}))  \\mathcal{R}(\\tau) \\Bigg] \\quad (Eq. 1.5) \\\\\n",
    "&=& \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T\\nabla_{\\theta}log(\\pi_{\\theta}(a_t| s_t))\\mathcal{R}(\\tau) \\right] \\quad (Eq. 1.6)\\\\\n",
    "&\\approx& \\frac{1}{|\\mathcal{D}_k|}\\sum_{\\tau \\in \\mathcal{D}_k}\\sum_{t=0}^T\\nabla_{\\theta}log(\\pi_{\\theta}(a_t| s_t))|_{\\theta_k}\\mathcal{R}(\\tau) \\quad (Eq. 1.7).\n",
    "\\end{eqnarray}\n",
    "\n",
    "$\\quad$\n",
    "\n",
    "- Eq. 1.2 uses the definition of the expected return, noting that $\\mathbb{E} [\\cdot] = \\int \\mathbb{P}(\\cdot) [\\cdot]$.\n",
    "\n",
    "<div style=\"background-color: yellow; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "**Expected value (a.k.a population mean or weighted average)  of a distribution:**\n",
    "\n",
    "- Expected value for **discrete variables**:\n",
    "    $$\\mu \\doteq  \\mathbb{E}[X]  \\doteq \\langle X\\rangle = \\sum_{j=1}^{\\dim \\Omega = d} x_jP(x_j).$$\n",
    "    The expected value is not the most likely value of $X$ and may not even be a possible value of $X$, but it is bound by\n",
    "    $$X_{\\min} \\leq\\langle X\\rangle\\leq X_{\\max}.$$\n",
    "\n",
    "- Expected value for **continuous variables**:\n",
    "    $$\\langle X \\rangle = \\int_{-\\infty}^{+\\infty} x \\rho(x) dx.$$\n",
    "  \n",
    "</div>\n",
    "  \n",
    "- Eq. 1.3 uses the [Leibniz integral rule](https://en.wikipedia.org/wiki/Leibniz_integral_rule) to bring the gradient symbol under the integral sign. This is possible because the integration domain (over $\\tau$) does not depend on $\\theta$. \n",
    "  \n",
    "- Eq. 1.4 uses the log-derivative trick:\n",
    "\n",
    "  \\begin{eqnarray}\\\n",
    "  \\frac{d}{dx}ln(f(x)) &=& \\frac{1}{f(x)}\\frac{d}{dx}f(x). \\\\\n",
    "                         &\\rightarrow&  \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\pi_{\\theta})) = \\frac{1}{\\mathbb{P}(\\tau | \\pi_{\\theta})}  \\nabla_{\\theta} \\mathbb{P}(\\tau | \\pi_{\\theta})\\\\\n",
    "                       &\\rightarrow&  \\nabla_{\\theta} \\mathbb{P}(\\tau | \\pi_{\\theta}) =  \\mathbb{P}(\\tau | \\pi_{\\theta}) \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\pi_{\\theta}))\n",
    "  \\end{eqnarray}\n",
    "\n",
    "- Eq. 1.5 is the expectation form of Eq. 1.4 using $\\mathbb{E} [\\cdot] = \\int \\mathbb{P}(\\cdot) [\\cdot]$.\n",
    "\n",
    "- Eq. 1.6 computes the expected value of the **gradient of the log-probability of a trajectory**, weighted by the return $\\mathcal{R}$, over all trajectories sampled i.i.d from the policy.\n",
    "\n",
    "<div style=\"background-color: yellow; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "**Derivation:**\n",
    "\n",
    "  \\begin{eqnarray}\n",
    "   \\nabla_{\\theta} log(\\mathbb{P}(\\tau | \\pi_{\\theta})) &=& \\nabla_{\\theta} log \\Bigg( \\rho_0(s_0) \\prod_{t=0}^{T} \\mathbb{P}(s_{t+1}|s_t,a_t) \\cdot \\pi(a_t | s_t) \\Bigg) \\\\\n",
    "   &=& \\nabla_{\\theta} \\Bigg[log (\\rho_0(s_0)) + log \\left( \\prod_{t=0}^{T} \\mathbb{P}(s_{t+1}|s_t,a_t) \\cdot \\pi(a_t | s_t) \\right) \\Bigg]\\\\\n",
    "   &=& \\nabla_{\\theta} \\Bigg[log (\\rho_0(s_0)) + \\sum_{t=0}^{T} log \\Big( \\mathbb{P}(s_{t+1}|s_t,a_t) \\cdot \\pi(a_t | s_t) \\Big) \\Bigg]\\\\\n",
    "   &=& \\nabla_{\\theta} \\Bigg[log (\\rho_0(s_0)) + \\sum_{t=0}^{T} log \\Big(\\mathbb{P}(s_{t+1}|s_t,a_t)\\Big) + log \\Big(\\pi(a_t | s_t)\\Big) \\Bigg]\\\\\n",
    "   &=& \\Bigg[\\cancel{\\nabla_{\\theta} log (\\rho_0(s_0))} + \\sum_{t=0}^{T} \\cancel{\\nabla_{\\theta} log \\Big(\\mathbb{P}(s_{t+1}|s_t,a_t)\\Big)} + \\nabla_{\\theta} log \\Big(\\pi(a_t | s_t)\\Big) \\Bigg]\\\\\n",
    "   &=& \\sum_{t=0}^T\\nabla_{\\theta}log(\\pi_{\\theta}(a_t| s_t))\n",
    "  \\end{eqnarray}\n",
    "</div>\n",
    "\n",
    "- Eq. 1.7 computes the empirical average (approximation) of Eq. 1.6. It is an unbiased estimator (sample mean) of the expectation $\\mathbb{E}_{\\tau \\sim \\pi_{\\theta_k}} [\\cdot]$ since trajectories are sampled i.i.d from the policy. As the number of trajectories $|\\mathcal{D}_k| \\rightarrow \\infty$, Eq. 1.7 converges to Eq. 1.6 due to the Law of Large Numbers.\n",
    " \n",
    "Legend:\n",
    "\n",
    "- $\\mathcal{D}_k$ denotes the set with a number $|\\mathcal{D}_k|$ of trajectories sampled i.i.d from the Policy $\\pi_k$ in the $k$-th iteration.\n",
    "- i.i.d means independent and identically distributed.\n",
    "    - Independent: the outcome of one random variable does not affect the outcomes of the others.\n",
    "    - Identically Distributed: all random variables in the collection follow the same probability distribution (e.g., same mean and variance). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "ZmuVV1z6h37w",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7.3 Actor-Critic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "6GnF-bXUh37w"
   },
   "source": [
    "Actor-Critic (AC) algorithms are a type of temporal difference learning method that make use of two neural networks as expressive nonlinear function approximators:\n",
    "\n",
    "- **Actor network:** learns/estimates the `parameterized Policy function` $\\pi_{\\theta_t}$ (**target policy**). It is a mapping from state to actions that is approximated/represented by a `neural network` as the expressive nonlinear function approximator. The goal is to update/optimize the Policy parameters $\\theta_t$ using `gradient ascent` in order to `maximize the expected cumulative reward (a.k.a expected return)` provided as feedback from the Critic network.\n",
    "<br>\n",
    "\n",
    "- **Critic network:** uses policy evaluation (e.g. `Temporal Difference learning`) to estimate either the Action-Value function $Q^{\\pi}(s_t, a_t)$, the State-Value function $V^{\\pi}(s_t)$, or the advantage function $A^{\\pi}(s_t, a_t)$ under Policy $\\pi$. It is trained using `gradient descent` to minimize the difference between its estimated values and the actual observed returns.\n",
    "<br>\n",
    "\n",
    "**Note1:** temporal difference means computing the difference of estimated values between successive states.\n",
    "\n",
    "**Note2:** The critic network is not the Behaviour Policy $b$.\n",
    "   \n",
    "Examples of AC algorithms are: A2C, A3C, DDPG, D4PG, MADDPG, TRPO, PPO, TD3, SAC, ACER, ACTKR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actor Network:\n",
    "    - Input: the input to the Actor network is typically the vector representing the `current state` of the environment.\n",
    "    - Output: the output of the Actor network is a vector of `actions`.\n",
    "\n",
    "- Critic Network:\n",
    "    - Input: the input to the Critic network is a combination of the `current state vector` (sometimes reward) and the output `action vector` from the Actor. These inputs are concatenated or combined in some way to form a joint representation.\n",
    "    - Output: the output of the Critic network is the `estimated action-value` or Q-value associated with the given state-action pair. It represents the expected cumulative reward when taking the given action in the given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "LGkt1IXfEkpZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8. Avoiding memory bottlenecks in deep neural networks <a name='memory' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prune the neural network model to have less hyperparameters.\n",
    "\n",
    "\n",
    "2. Quantize the neural network model to have less precision by working with int8 data type instead of float32. Floating point operations cost more than integers.Â Recall that `np.int8` uses 1 byte and can hold values from -128 to 127, and `np.uint8v` uses 1 byte and hold values from 0 to 255, while `np.float32` uses 4 bytes.\n",
    "\n",
    "\n",
    "3. Read the dataset from an external HD in batches using python Generator objects instead of loading everything into ram memory (such as in a numpy ndarray, in a set, or worse in a list). There is a tradeoff since computations with list comprehension are faster than with generators. And there's an added overhead from moving data from the HD to the ram memory since this will slow down training.\n",
    "\n",
    "\n",
    "4. Run experiment with cloud computing: [Google Colab](https://colab.research.google.com/), [Amazon Web Services (AWS)](https://aws.amazon.com/braket/), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "sQM0iL07uSpa",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# References &nbsp; <a href=\"#\"><img valign=\"middle\" height=\"45px\" src=\"https://img.icons8.com/book\" width=\"45\" hspace=\"0px\" vspace=\"0px\"></a><a name=\"References\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "g30_SRcouTPq"
   },
   "source": [
    "<a name=\"1\"></a> \\[1] \"Reinforcement Learning: An Introduction.\" Richard S. Sutton and Andrew G. Barto. [Cambridge, MA: The MIT Press, March 22, 2018, 548 pp.](#).\n",
    "    \n",
    "- Complementary material: \n",
    "    - [UCL COMPM050/COMPGI13 (2015): RL by David Silver](https://www.davidsilver.uk/teaching/).\n",
    "    - [UC Berkeley CS285 (Fall 2022): Deep RL by Sergey Levine](http://rail.eecs.berkeley.edu/deeprlcourse/).\n",
    "    - [Deep RL Bootcamp, Berkeley CA](https://sites.google.com/view/deep-rl-bootcamp/lectures).\n",
    "    - [John Schulman's PhD Thesis](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-217.html).\n",
    "    - [Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/).\n",
    "    - [Williams, R.J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach Learn 8, 229â€“256 (1992).](https://doi.org/10.1007/BF00992696)\n",
    "\n",
    "<a name=\"2\"></a> \\[2] Deep Learning (Ian J. Goodfellow, Yoshua Bengio and Aaron Courville), MIT Press, 2016."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-Btwr1p8uYLZ",
    "_GDi1bB97XWW",
    "vAfth5BJkhRE",
    "v-Vthkrgh36f",
    "-hmtxjzEh36i",
    "fyPQE97zh36i",
    "iFmzvjpDh36i",
    "N0OC3Xlih36j",
    "WWHAkpcrh36k",
    "atteyA1mh36l",
    "sEjDKD8th36l",
    "dsbtRlGkh36m",
    "sIDun499h36n",
    "Uaqm3w40h36r",
    "7ppO_NXVh36s",
    "A82rHjoVh36s",
    "9JSZ-qo9h36t",
    "AtlaQroWh36u",
    "yB9V4Ks_h36v",
    "1C-JntfKh36w",
    "aiD8CTOxh36y",
    "94i0GqZlh36z",
    "sd8-_RiMh36z",
    "NzJrv9sph360",
    "G56Z-US2h360",
    "-pA2WLBIJjPf",
    "baSeSMpzWcHa",
    "QyNmEIRnh363",
    "gisL9_D8h369",
    "p_lBrVFZh37t",
    "mieCj6uUh37u",
    "Jk7E8Yr9h37u",
    "km7e-PrPh37v",
    "ZmuVV1z6h37w",
    "5xSl02hYh37w",
    "lhZ7tbbjh37w",
    "B6u884Qgh37x",
    "LGkt1IXfEkpZ",
    "sQM0iL07uSpa"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "187.797px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
